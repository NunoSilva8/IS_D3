[2021-12-08 12:15:55,258] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:68)
[2021-12-08 12:15:55,270] INFO WorkerInfo values: 
	jvm.args = -Xmx256M, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=C:\kafka/logs, -Dlog4j.configuration=file:C:\kafka/config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 17.0.1, 17.0.1+12-LTS-39
	jvm.classpath = C:\kafka\libs\activation-1.1.1.jar;C:\kafka\libs\aopalliance-repackaged-2.6.1.jar;C:\kafka\libs\argparse4j-0.7.0.jar;C:\kafka\libs\audience-annotations-0.5.0.jar;C:\kafka\libs\commons-cli-1.4.jar;C:\kafka\libs\commons-lang3-3.8.1.jar;C:\kafka\libs\connect-api-2.8.1.jar;C:\kafka\libs\connect-basic-auth-extension-2.8.1.jar;C:\kafka\libs\connect-file-2.8.1.jar;C:\kafka\libs\connect-json-2.8.1.jar;C:\kafka\libs\connect-mirror-2.8.1.jar;C:\kafka\libs\connect-mirror-client-2.8.1.jar;C:\kafka\libs\connect-runtime-2.8.1.jar;C:\kafka\libs\connect-transforms-2.8.1.jar;C:\kafka\libs\hk2-api-2.6.1.jar;C:\kafka\libs\hk2-locator-2.6.1.jar;C:\kafka\libs\hk2-utils-2.6.1.jar;C:\kafka\libs\jackson-annotations-2.10.5.jar;C:\kafka\libs\jackson-core-2.10.5.jar;C:\kafka\libs\jackson-databind-2.10.5.1.jar;C:\kafka\libs\jackson-dataformat-csv-2.10.5.jar;C:\kafka\libs\jackson-datatype-jdk8-2.10.5.jar;C:\kafka\libs\jackson-jaxrs-base-2.10.5.jar;C:\kafka\libs\jackson-jaxrs-json-provider-2.10.5.jar;C:\kafka\libs\jackson-module-jaxb-annotations-2.10.5.jar;C:\kafka\libs\jackson-module-paranamer-2.10.5.jar;C:\kafka\libs\jackson-module-scala_2.12-2.10.5.jar;C:\kafka\libs\jakarta.activation-api-1.2.1.jar;C:\kafka\libs\jakarta.annotation-api-1.3.5.jar;C:\kafka\libs\jakarta.inject-2.6.1.jar;C:\kafka\libs\jakarta.validation-api-2.0.2.jar;C:\kafka\libs\jakarta.ws.rs-api-2.1.6.jar;C:\kafka\libs\jakarta.xml.bind-api-2.3.2.jar;C:\kafka\libs\javassist-3.27.0-GA.jar;C:\kafka\libs\javax.servlet-api-3.1.0.jar;C:\kafka\libs\javax.ws.rs-api-2.1.1.jar;C:\kafka\libs\jaxb-api-2.3.0.jar;C:\kafka\libs\jersey-client-2.34.jar;C:\kafka\libs\jersey-common-2.34.jar;C:\kafka\libs\jersey-container-servlet-2.34.jar;C:\kafka\libs\jersey-container-servlet-core-2.34.jar;C:\kafka\libs\jersey-hk2-2.34.jar;C:\kafka\libs\jersey-server-2.34.jar;C:\kafka\libs\jetty-client-9.4.43.v20210629.jar;C:\kafka\libs\jetty-continuation-9.4.43.v20210629.jar;C:\kafka\libs\jetty-http-9.4.43.v20210629.jar;C:\kafka\libs\jetty-io-9.4.43.v20210629.jar;C:\kafka\libs\jetty-security-9.4.43.v20210629.jar;C:\kafka\libs\jetty-server-9.4.43.v20210629.jar;C:\kafka\libs\jetty-servlet-9.4.43.v20210629.jar;C:\kafka\libs\jetty-servlets-9.4.43.v20210629.jar;C:\kafka\libs\jetty-util-9.4.43.v20210629.jar;C:\kafka\libs\jetty-util-ajax-9.4.43.v20210629.jar;C:\kafka\libs\jline-3.12.1.jar;C:\kafka\libs\jopt-simple-5.0.4.jar;C:\kafka\libs\kafka-clients-2.8.1.jar;C:\kafka\libs\kafka-connect-jdbc-10.2.5.jar;C:\kafka\libs\kafka-log4j-appender-2.8.1.jar;C:\kafka\libs\kafka-metadata-2.8.1.jar;C:\kafka\libs\kafka-raft-2.8.1.jar;C:\kafka\libs\kafka-shell-2.8.1.jar;C:\kafka\libs\kafka-streams-2.8.1.jar;C:\kafka\libs\kafka-streams-examples-2.8.1.jar;C:\kafka\libs\kafka-streams-scala_2.12-2.8.1.jar;C:\kafka\libs\kafka-streams-test-utils-2.8.1.jar;C:\kafka\libs\kafka-tools-2.8.1.jar;C:\kafka\libs\kafka_2.12-2.8.1-javadoc.jar;C:\kafka\libs\kafka_2.12-2.8.1-javadoc.jar.asc;C:\kafka\libs\kafka_2.12-2.8.1-sources.jar;C:\kafka\libs\kafka_2.12-2.8.1-sources.jar.asc;C:\kafka\libs\kafka_2.12-2.8.1-test-sources.jar;C:\kafka\libs\kafka_2.12-2.8.1-test-sources.jar.asc;C:\kafka\libs\kafka_2.12-2.8.1-test.jar;C:\kafka\libs\kafka_2.12-2.8.1-test.jar.asc;C:\kafka\libs\kafka_2.12-2.8.1.jar;C:\kafka\libs\kafka_2.12-2.8.1.jar.asc;C:\kafka\libs\log4j-1.2.17.jar;C:\kafka\libs\lz4-java-1.7.1.jar;C:\kafka\libs\maven-artifact-3.8.1.jar;C:\kafka\libs\metrics-core-2.2.0.jar;C:\kafka\libs\mysql-connector-java-8.0.27.jar;C:\kafka\libs\netty-buffer-4.1.62.Final.jar;C:\kafka\libs\netty-codec-4.1.62.Final.jar;C:\kafka\libs\netty-common-4.1.62.Final.jar;C:\kafka\libs\netty-handler-4.1.62.Final.jar;C:\kafka\libs\netty-resolver-4.1.62.Final.jar;C:\kafka\libs\netty-transport-4.1.62.Final.jar;C:\kafka\libs\netty-transport-native-epoll-4.1.62.Final.jar;C:\kafka\libs\netty-transport-native-unix-common-4.1.62.Final.jar;C:\kafka\libs\osgi-resource-locator-1.0.3.jar;C:\kafka\libs\paranamer-2.8.jar;C:\kafka\libs\plexus-utils-3.2.1.jar;C:\kafka\libs\reflections-0.9.12.jar;C:\kafka\libs\rocksdbjni-5.18.4.jar;C:\kafka\libs\scala-collection-compat_2.12-2.3.0.jar;C:\kafka\libs\scala-java8-compat_2.12-0.9.1.jar;C:\kafka\libs\scala-library-2.12.13.jar;C:\kafka\libs\scala-logging_2.12-3.9.2.jar;C:\kafka\libs\scala-reflect-2.12.13.jar;C:\kafka\libs\slf4j-api-1.7.30.jar;C:\kafka\libs\slf4j-log4j12-1.7.30.jar;C:\kafka\libs\snappy-java-1.1.8.1.jar;C:\kafka\libs\zookeeper-3.5.9.jar;C:\kafka\libs\zookeeper-jute-3.5.9.jar;C:\kafka\libs\zstd-jni-1.4.9-1.jar
	os.spec = Windows 10, amd64, 10.0
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2021-12-08 12:15:55,282] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectStandalone:77)
[2021-12-08 12:15:56,800] INFO Registered loader: jdk.internal.loader.ClassLoaders$AppClassLoader@266474c2 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2021-12-08 12:15:56,800] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,802] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,805] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,806] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,807] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,807] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,808] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,808] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,809] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,810] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,811] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,812] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,814] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,815] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,815] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,816] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,816] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,818] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,820] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,821] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,821] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,822] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,823] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,823] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,824] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,824] INFO Added plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,825] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,830] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,830] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,832] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,832] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,833] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,834] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,834] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,835] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,835] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,836] INFO Added plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,836] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,837] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,838] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,838] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,839] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,839] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,840] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,840] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,841] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,846] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,847] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,847] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,848] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,849] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,849] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,850] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:15:56,859] INFO Added aliases 'JdbcSinkConnector' and 'JdbcSink' to plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:15:56,861] INFO Added aliases 'JdbcSourceConnector' and 'JdbcSource' to plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:15:56,862] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:15:56,864] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:15:56,865] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:15:56,866] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:15:56,866] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:15:56,867] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:15:56,868] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:15:56,868] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:15:56,869] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:15:56,870] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:15:56,871] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:15:56,871] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:15:56,872] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:15:56,875] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:15:56,876] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:15:56,877] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:15:56,880] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:15:56,880] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:15:56,881] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:15:56,881] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:15:56,882] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:15:56,882] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:15:56,883] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:15:56,884] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:15:56,886] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:15:56,887] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:15:56,887] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-12-08 12:15:56,888] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:15:56,893] INFO Added aliases 'PredicatedTransformation' and 'Predicated' to plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:15:56,894] INFO Added alias 'Filter' to plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-12-08 12:15:56,896] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-12-08 12:15:56,897] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-12-08 12:15:56,897] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-12-08 12:15:56,898] INFO Added alias 'HasHeaderKey' to plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-12-08 12:15:56,899] INFO Added alias 'RecordIsTombstone' to plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-12-08 12:15:56,899] INFO Added alias 'TopicNameMatches' to plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-12-08 12:15:56,900] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-12-08 12:15:56,900] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:15:56,901] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:15:56,901] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:15:56,927] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	config.providers = []
	connector.client.config.override.policy = None
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	listeners = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = /tmp/connect.offsets
	plugin.path = null
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = null
	rest.port = 8083
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:372)
[2021-12-08 12:15:56,929] WARN Variables cannot be used in the 'plugin.path' property, since the property is used by plugin scanning before the config providers that replace the variables are initialized. The raw value 'null' was used for plugin scanning, as opposed to the transformed value 'null', and this may cause unexpected results. (org.apache.kafka.connect.runtime.WorkerConfig:420)
[2021-12-08 12:15:56,935] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2021-12-08 12:15:56,941] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:372)
[2021-12-08 12:15:57,077] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:15:57,078] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:15:57,080] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:15:57,083] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:15:57,084] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:15:57,084] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:15:57,086] INFO Kafka version: 2.8.1 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-12-08 12:15:57,086] INFO Kafka commitId: 839b886f9b732b15 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-12-08 12:15:57,087] INFO Kafka startTimeMs: 1638965757085 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-12-08 12:15:57,485] INFO Kafka cluster ID: u7MsxwDfR5Ob7FBWXv2wvA (org.apache.kafka.connect.util.ConnectUtils:65)
[2021-12-08 12:15:57,487] INFO App info kafka.admin.client for adminclient-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-12-08 12:15:57,500] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2021-12-08 12:15:57,500] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2021-12-08 12:15:57,501] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2021-12-08 12:15:57,521] INFO Logging initialized @2757ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:170)
[2021-12-08 12:15:57,591] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:132)
[2021-12-08 12:15:57,592] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:204)
[2021-12-08 12:15:57,602] INFO jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 17.0.1+12-LTS-39 (org.eclipse.jetty.server.Server:375)
[2021-12-08 12:15:57,640] INFO Started http_8083@91c4a3f{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:331)
[2021-12-08 12:15:57,640] INFO Started @2877ms (org.eclipse.jetty.server.Server:415)
[2021-12-08 12:15:57,687] INFO Advertised URI: http://169.254.211.170:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:371)
[2021-12-08 12:15:57,687] INFO REST server listening at http://169.254.211.170:8083/, advertising URL http://169.254.211.170:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2021-12-08 12:15:57,690] INFO Advertised URI: http://169.254.211.170:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:371)
[2021-12-08 12:15:57,692] INFO REST admin endpoints at http://169.254.211.170:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:220)
[2021-12-08 12:15:57,694] INFO Advertised URI: http://169.254.211.170:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:371)
[2021-12-08 12:15:57,697] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy:45)
[2021-12-08 12:15:57,704] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2021-12-08 12:15:57,707] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:372)
[2021-12-08 12:15:57,723] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:15:57,723] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:15:57,724] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:15:57,726] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:15:57,728] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:15:57,729] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:15:57,729] INFO Kafka version: 2.8.1 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-12-08 12:15:57,730] INFO Kafka commitId: 839b886f9b732b15 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-12-08 12:15:57,731] INFO Kafka startTimeMs: 1638965757729 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-12-08 12:15:57,756] INFO Kafka cluster ID: u7MsxwDfR5Ob7FBWXv2wvA (org.apache.kafka.connect.util.ConnectUtils:65)
[2021-12-08 12:15:57,757] INFO App info kafka.admin.client for adminclient-2 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-12-08 12:15:57,767] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2021-12-08 12:15:57,768] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2021-12-08 12:15:57,769] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2021-12-08 12:15:57,773] INFO Kafka version: 2.8.1 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-12-08 12:15:57,773] INFO Kafka commitId: 839b886f9b732b15 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-12-08 12:15:57,774] INFO Kafka startTimeMs: 1638965757773 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-12-08 12:15:57,900] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2021-12-08 12:15:57,903] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2021-12-08 12:15:57,913] INFO Kafka Connect standalone worker initialization took 2651ms (org.apache.kafka.connect.cli.ConnectStandalone:99)
[2021-12-08 12:15:57,914] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2021-12-08 12:15:57,915] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:94)
[2021-12-08 12:15:57,915] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:191)
[2021-12-08 12:15:57,917] INFO Starting FileOffsetBackingStore with file \tmp\connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:58)
[2021-12-08 12:15:57,922] INFO Worker started (org.apache.kafka.connect.runtime.Worker:198)
[2021-12-08 12:15:57,922] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:97)
[2021-12-08 12:15:57,923] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:224)
[2021-12-08 12:15:57,961] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:241)
[2021-12-08 12:15:58,039] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:334)
[2021-12-08 12:15:58,039] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:339)
[2021-12-08 12:15:58,042] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session:132)
[2021-12-08 12:15:58,558] INFO Started o.e.j.s.ServletContextHandler@53ac845a{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:915)
[2021-12-08 12:15:58,560] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:319)
[2021-12-08 12:15:58,560] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2021-12-08 12:15:58,583] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:372)
[2021-12-08 12:15:58,599] INFO Creating connector jdbc-source-clients of type io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:271)
[2021-12-08 12:15:58,599] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-clients
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:372)
[2021-12-08 12:15:58,601] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-clients
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:15:58,607] INFO Instantiated connector jdbc-source-clients with version 10.2.5 of type class io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:281)
[2021-12-08 12:15:58,607] INFO Finished creating connector jdbc-source-clients (org.apache.kafka.connect.runtime.Worker:306)
[2021-12-08 12:15:58,609] INFO Starting JDBC Source Connector (io.confluent.connect.jdbc.JdbcSourceConnector:69)
[2021-12-08 12:15:58,610] INFO JdbcSourceConnectorConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:mysql://localhost:3306/project3
	connection.user = projeto
	db.timezone = UTC
	dialect.name = MySqlDatabaseDialect
	incrementing.column.name = 
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 60000
	query = SELECT * FROM clients;
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = []
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = []
	timestamp.delay.interval.ms = 0
	timestamp.initial = null
	topic.prefix = DBinfo
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceConnectorConfig:372)
[2021-12-08 12:15:58,615] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2021-12-08 12:15:58,766] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-clients
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:372)
[2021-12-08 12:15:58,767] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-clients
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:15:58,773] INFO Creating task jdbc-source-clients-0 (org.apache.kafka.connect.runtime.Worker:505)
[2021-12-08 12:15:58,775] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-clients
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:372)
[2021-12-08 12:15:58,777] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-clients
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:15:58,779] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.source.JdbcSourceTask
 (org.apache.kafka.connect.runtime.TaskConfig:372)
[2021-12-08 12:15:58,779] INFO Instantiated task jdbc-source-clients-0 with version 10.2.5 of type io.confluent.connect.jdbc.source.JdbcSourceTask (org.apache.kafka.connect.runtime.Worker:520)
[2021-12-08 12:15:58,780] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2021-12-08 12:15:58,780] INFO Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-source-clients-0 using the worker config (org.apache.kafka.connect.runtime.Worker:533)
[2021-12-08 12:15:58,781] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2021-12-08 12:15:58,782] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-source-clients-0 using the worker config (org.apache.kafka.connect.runtime.Worker:539)
[2021-12-08 12:15:58,782] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-source-clients-0 using the worker config (org.apache.kafka.connect.runtime.Worker:546)
[2021-12-08 12:15:58,789] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-clients
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:372)
[2021-12-08 12:15:58,790] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-clients
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:15:58,793] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:600)
[2021-12-08 12:15:58,798] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-jdbc-source-clients-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:372)
[2021-12-08 12:15:58,834] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-12-08 12:15:58,834] INFO Kafka version: 2.8.1 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-12-08 12:15:58,836] INFO Kafka commitId: 839b886f9b732b15 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-12-08 12:15:58,841] INFO Kafka startTimeMs: 1638965758834 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-12-08 12:15:58,856] INFO [Producer clientId=connector-producer-jdbc-source-clients-0] Cluster ID: u7MsxwDfR5Ob7FBWXv2wvA (org.apache.kafka.clients.Metadata:279)
[2021-12-08 12:15:58,862] INFO Starting JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:88)
[2021-12-08 12:15:58,868] INFO Created connector jdbc-source-clients (org.apache.kafka.connect.cli.ConnectStandalone:109)
[2021-12-08 12:15:58,868] INFO JdbcSourceTaskConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:mysql://localhost:3306/project3
	connection.user = projeto
	db.timezone = UTC
	dialect.name = MySqlDatabaseDialect
	incrementing.column.name = 
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 60000
	query = SELECT * FROM clients;
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = []
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	tables = []
	timestamp.column.name = []
	timestamp.delay.interval.ms = 0
	timestamp.initial = null
	topic.prefix = DBinfo
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceTaskConfig:372)
[2021-12-08 12:15:58,871] INFO Using JDBC dialect MySql (io.confluent.connect.jdbc.source.JdbcSourceTask:105)
[2021-12-08 12:15:58,871] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:372)
[2021-12-08 12:15:58,874] INFO Creating connector jdbc-source-managers of type io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:271)
[2021-12-08 12:15:58,876] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-managers
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:372)
[2021-12-08 12:15:58,878] INFO Started JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:264)
[2021-12-08 12:15:58,882] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-managers
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:15:58,883] INFO WorkerSourceTask{id=jdbc-source-clients-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:232)
[2021-12-08 12:15:58,888] INFO Instantiated connector jdbc-source-managers with version 10.2.5 of type class io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:281)
[2021-12-08 12:15:58,888] INFO WorkerSourceTask{id=jdbc-source-clients-0} Executing source task (org.apache.kafka.connect.runtime.WorkerSourceTask:238)
[2021-12-08 12:15:58,894] INFO Finished creating connector jdbc-source-managers (org.apache.kafka.connect.runtime.Worker:306)
[2021-12-08 12:15:58,896] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2021-12-08 12:15:58,895] INFO Starting JDBC Source Connector (io.confluent.connect.jdbc.JdbcSourceConnector:69)
[2021-12-08 12:15:58,901] INFO JdbcSourceConnectorConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:mysql://localhost:3306/project3
	connection.user = projeto
	db.timezone = UTC
	dialect.name = MySqlDatabaseDialect
	incrementing.column.name = 
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 60000
	query = SELECT * FROM managers;
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = []
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = []
	timestamp.delay.interval.ms = 0
	timestamp.initial = null
	topic.prefix = DBinfo
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceConnectorConfig:372)
[2021-12-08 12:15:58,902] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2021-12-08 12:15:58,908] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-managers
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:372)
[2021-12-08 12:15:58,914] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-managers
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:15:58,915] INFO Creating task jdbc-source-managers-0 (org.apache.kafka.connect.runtime.Worker:505)
[2021-12-08 12:15:58,917] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-managers
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:372)
[2021-12-08 12:15:58,920] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-managers
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:15:58,921] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.source.JdbcSourceTask
 (org.apache.kafka.connect.runtime.TaskConfig:372)
[2021-12-08 12:15:58,922] INFO Instantiated task jdbc-source-managers-0 with version 10.2.5 of type io.confluent.connect.jdbc.source.JdbcSourceTask (org.apache.kafka.connect.runtime.Worker:520)
[2021-12-08 12:15:58,923] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2021-12-08 12:15:58,927] INFO Begin using SQL query: SELECT * FROM clients; (io.confluent.connect.jdbc.source.TableQuerier:164)
[2021-12-08 12:15:58,930] INFO Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-source-managers-0 using the worker config (org.apache.kafka.connect.runtime.Worker:533)
[2021-12-08 12:15:58,932] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2021-12-08 12:15:58,933] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-source-managers-0 using the worker config (org.apache.kafka.connect.runtime.Worker:539)
[2021-12-08 12:15:58,934] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-source-managers-0 using the worker config (org.apache.kafka.connect.runtime.Worker:546)
[2021-12-08 12:15:58,935] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-managers
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:372)
[2021-12-08 12:15:58,936] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-managers
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:15:58,937] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:600)
[2021-12-08 12:15:58,940] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-jdbc-source-managers-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:372)
[2021-12-08 12:15:58,957] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-12-08 12:15:58,958] INFO Kafka version: 2.8.1 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-12-08 12:15:58,960] INFO Kafka commitId: 839b886f9b732b15 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-12-08 12:15:58,963] INFO Kafka startTimeMs: 1638965758958 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-12-08 12:15:58,967] INFO Created connector jdbc-source-managers (org.apache.kafka.connect.cli.ConnectStandalone:109)
[2021-12-08 12:15:58,970] INFO [Producer clientId=connector-producer-jdbc-source-managers-0] Cluster ID: u7MsxwDfR5Ob7FBWXv2wvA (org.apache.kafka.clients.Metadata:279)
[2021-12-08 12:15:58,974] INFO Starting JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:88)
[2021-12-08 12:15:58,977] INFO JdbcSourceTaskConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:mysql://localhost:3306/project3
	connection.user = projeto
	db.timezone = UTC
	dialect.name = MySqlDatabaseDialect
	incrementing.column.name = 
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 60000
	query = SELECT * FROM managers;
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = []
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	tables = []
	timestamp.column.name = []
	timestamp.delay.interval.ms = 0
	timestamp.initial = null
	topic.prefix = DBinfo
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceTaskConfig:372)
[2021-12-08 12:15:58,982] INFO Using JDBC dialect MySql (io.confluent.connect.jdbc.source.JdbcSourceTask:105)
[2021-12-08 12:15:58,983] INFO Started JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:264)
[2021-12-08 12:15:58,987] INFO WorkerSourceTask{id=jdbc-source-managers-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:232)
[2021-12-08 12:15:58,990] ERROR Non-transient SQL exception while running query for table: BulkTableQuerier{table='null', query='SELECT * FROM clients;', topicPrefix='DBinfo'} (io.confluent.connect.jdbc.source.JdbcSourceTask:412)
java.sql.SQLSyntaxErrorException: Table 'project3.clients' doesn't exist
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeQuery(ClientPreparedStatement.java:1009)
	at io.confluent.connect.jdbc.source.BulkTableQuerier.executeQuery(BulkTableQuerier.java:79)
	at io.confluent.connect.jdbc.source.TableQuerier.maybeStartQuery(TableQuerier.java:98)
	at io.confluent.connect.jdbc.source.BulkTableQuerier.maybeStartQuery(BulkTableQuerier.java:39)
	at io.confluent.connect.jdbc.source.JdbcSourceTask.poll(JdbcSourceTask.java:379)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.poll(WorkerSourceTask.java:296)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.execute(WorkerSourceTask.java:253)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
[2021-12-08 12:15:58,989] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:372)
[2021-12-08 12:15:58,998] INFO Closing resources for JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:332)
[2021-12-08 12:15:58,993] INFO WorkerSourceTask{id=jdbc-source-managers-0} Executing source task (org.apache.kafka.connect.runtime.WorkerSourceTask:238)
[2021-12-08 12:15:58,998] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2021-12-08 12:15:58,998] INFO Creating connector jdbc-sink of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:271)
[2021-12-08 12:15:59,001] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:15:58,999] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2021-12-08 12:15:59,002] ERROR WorkerSourceTask{id=jdbc-source-clients-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:190)
org.apache.kafka.connect.errors.ConnectException: java.sql.SQLSyntaxErrorException: Table 'project3.clients' doesn't exist
	at io.confluent.connect.jdbc.source.JdbcSourceTask.poll(JdbcSourceTask.java:417)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.poll(WorkerSourceTask.java:296)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.execute(WorkerSourceTask.java:253)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLSyntaxErrorException: Table 'project3.clients' doesn't exist
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeQuery(ClientPreparedStatement.java:1009)
	at io.confluent.connect.jdbc.source.BulkTableQuerier.executeQuery(BulkTableQuerier.java:79)
	at io.confluent.connect.jdbc.source.TableQuerier.maybeStartQuery(TableQuerier.java:98)
	at io.confluent.connect.jdbc.source.BulkTableQuerier.maybeStartQuery(BulkTableQuerier.java:39)
	at io.confluent.connect.jdbc.source.JdbcSourceTask.poll(JdbcSourceTask.java:379)
	... 9 more
[2021-12-08 12:15:59,001] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	topics = [results]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2021-12-08 12:15:59,003] INFO Stopping JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:320)
[2021-12-08 12:15:59,005] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	topics = [results]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:15:59,005] INFO Closing resources for JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:332)
[2021-12-08 12:15:59,008] INFO Instantiated connector jdbc-sink with version 10.2.5 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:281)
[2021-12-08 12:15:59,008] INFO [Producer clientId=connector-producer-jdbc-source-clients-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1204)
[2021-12-08 12:15:59,010] INFO Finished creating connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:306)
[2021-12-08 12:15:59,014] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2021-12-08 12:15:59,014] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	topics = [results]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2021-12-08 12:15:59,015] INFO Begin using SQL query: SELECT * FROM managers; (io.confluent.connect.jdbc.source.TableQuerier:164)
[2021-12-08 12:15:59,014] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2021-12-08 12:15:59,025] ERROR Non-transient SQL exception while running query for table: BulkTableQuerier{table='null', query='SELECT * FROM managers;', topicPrefix='DBinfo'} (io.confluent.connect.jdbc.source.JdbcSourceTask:412)
java.sql.SQLSyntaxErrorException: Table 'project3.managers' doesn't exist
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeQuery(ClientPreparedStatement.java:1009)
	at io.confluent.connect.jdbc.source.BulkTableQuerier.executeQuery(BulkTableQuerier.java:79)
	at io.confluent.connect.jdbc.source.TableQuerier.maybeStartQuery(TableQuerier.java:98)
	at io.confluent.connect.jdbc.source.BulkTableQuerier.maybeStartQuery(BulkTableQuerier.java:39)
	at io.confluent.connect.jdbc.source.JdbcSourceTask.poll(JdbcSourceTask.java:379)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.poll(WorkerSourceTask.java:296)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.execute(WorkerSourceTask.java:253)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
[2021-12-08 12:15:59,023] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	topics = [results]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:15:59,027] INFO Closing resources for JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:332)
[2021-12-08 12:15:59,025] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2021-12-08 12:15:59,029] INFO Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:44)
[2021-12-08 12:15:59,029] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2021-12-08 12:15:59,030] INFO Creating task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:505)
[2021-12-08 12:15:59,030] INFO App info kafka.producer for connector-producer-jdbc-source-clients-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-12-08 12:15:59,031] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:15:59,037] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:372)
[2021-12-08 12:15:59,037] ERROR WorkerSourceTask{id=jdbc-source-managers-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:190)
org.apache.kafka.connect.errors.ConnectException: java.sql.SQLSyntaxErrorException: Table 'project3.managers' doesn't exist
	at io.confluent.connect.jdbc.source.JdbcSourceTask.poll(JdbcSourceTask.java:417)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.poll(WorkerSourceTask.java:296)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.execute(WorkerSourceTask.java:253)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLSyntaxErrorException: Table 'project3.managers' doesn't exist
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeQuery(ClientPreparedStatement.java:1009)
	at io.confluent.connect.jdbc.source.BulkTableQuerier.executeQuery(BulkTableQuerier.java:79)
	at io.confluent.connect.jdbc.source.TableQuerier.maybeStartQuery(TableQuerier.java:98)
	at io.confluent.connect.jdbc.source.BulkTableQuerier.maybeStartQuery(BulkTableQuerier.java:39)
	at io.confluent.connect.jdbc.source.JdbcSourceTask.poll(JdbcSourceTask.java:379)
	... 9 more
[2021-12-08 12:15:59,039] INFO Stopping JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:320)
[2021-12-08 12:15:59,038] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:15:59,040] INFO Closing resources for JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:332)
[2021-12-08 12:15:59,041] INFO [Producer clientId=connector-producer-jdbc-source-managers-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1204)
[2021-12-08 12:15:59,041] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:372)
[2021-12-08 12:15:59,045] INFO Instantiated task jdbc-sink-0 with version 10.2.5 of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:520)
[2021-12-08 12:15:59,046] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2021-12-08 12:15:59,049] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:372)
[2021-12-08 12:15:59,053] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2021-12-08 12:15:59,055] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2021-12-08 12:15:59,056] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2021-12-08 12:15:59,056] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-sink-0 using the connector config (org.apache.kafka.connect.runtime.Worker:535)
[2021-12-08 12:15:59,057] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-sink-0 using the connector config (org.apache.kafka.connect.runtime.Worker:541)
[2021-12-08 12:15:59,057] INFO App info kafka.producer for connector-producer-jdbc-source-managers-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-12-08 12:15:59,058] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:546)
[2021-12-08 12:15:59,062] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:626)
[2021-12-08 12:15:59,064] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	topics = [results]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2021-12-08 12:15:59,065] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	topics = [results]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:15:59,072] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-jdbc-sink-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-jdbc-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:372)
[2021-12-08 12:15:59,106] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-12-08 12:15:59,107] INFO Kafka version: 2.8.1 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-12-08 12:15:59,113] INFO Kafka commitId: 839b886f9b732b15 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-12-08 12:15:59,113] INFO Kafka startTimeMs: 1638965759107 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-12-08 12:15:59,120] INFO Created connector jdbc-sink (org.apache.kafka.connect.cli.ConnectStandalone:109)
[2021-12-08 12:15:59,122] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Subscribed to topic(s): results (org.apache.kafka.clients.consumer.KafkaConsumer:965)
[2021-12-08 12:15:59,123] INFO Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:48)
[2021-12-08 12:15:59,124] INFO JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = false
	batch.size = 3000
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:mysql://localhost:3306/project3
	connection.user = projeto
	db.timezone = UTC
	delete.enabled = false
	dialect.name = MySqlDatabaseDialect
	fields.whitelist = []
	insert.mode = insert
	max.retries = 10
	pk.fields = []
	pk.mode = none
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = ${topic}
	table.types = [TABLE]
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:372)
[2021-12-08 12:15:59,131] INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:67)
[2021-12-08 12:15:59,132] INFO WorkerSinkTask{id=jdbc-sink-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:310)
[2021-12-08 12:15:59,133] INFO WorkerSinkTask{id=jdbc-sink-0} Executing sink task (org.apache.kafka.connect.runtime.WorkerSinkTask:196)
[2021-12-08 12:15:59,197] WARN [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Error while fetching metadata with correlation id 2 : {results=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1100)
[2021-12-08 12:15:59,200] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Cluster ID: u7MsxwDfR5Ob7FBWXv2wvA (org.apache.kafka.clients.Metadata:279)
[2021-12-08 12:15:59,207] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Discovered group coordinator Nuno-PC:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:850)
[2021-12-08 12:15:59,213] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-12-08 12:15:59,267] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-12-08 12:15:59,327] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Successfully joined group with generation Generation{generationId=3, memberId='connector-consumer-jdbc-sink-0-6fee5263-1355-490f-b84c-f1bbf6439dc0', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-12-08 12:15:59,332] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Finished assignment for group at generation 3: {connector-consumer-jdbc-sink-0-6fee5263-1355-490f-b84c-f1bbf6439dc0=Assignment(partitions=[results-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:626)
[2021-12-08 12:15:59,438] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Successfully synced group in generation Generation{generationId=3, memberId='connector-consumer-jdbc-sink-0-6fee5263-1355-490f-b84c-f1bbf6439dc0', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:760)
[2021-12-08 12:15:59,440] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Notifying assignor about the new Assignment(partitions=[results-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-12-08 12:15:59,446] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Adding newly assigned partitions: results-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-12-08 12:15:59,468] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Found no committed offset for partition results-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1362)
[2021-12-08 12:15:59,496] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Resetting offset for partition results-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[Nuno-PC:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2021-12-08 12:16:08,871] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:16:08,980] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:16:18,884] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:16:18,994] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:16:28,891] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:16:29,000] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:16:38,898] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:16:39,009] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:16:48,909] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:16:49,018] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:16:58,921] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:16:59,030] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:17:08,923] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:17:09,031] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:17:18,924] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:17:19,033] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:17:28,934] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:17:29,042] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:17:34,439] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2021-12-08 12:17:34,442] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2021-12-08 12:17:34,464] INFO Checking MySql dialect for existence of TABLE "results" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:575)
[2021-12-08 12:17:34,476] INFO Using MySql dialect TABLE "results" absent (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:583)
[2021-12-08 12:17:34,478] INFO Creating table with sql: CREATE TABLE `results` (
`revenue` DOUBLE NOT NULL,
`expenses` DOUBLE NOT NULL,
`profit` DOUBLE NOT NULL) (io.confluent.connect.jdbc.sink.DbStructure:122)
[2021-12-08 12:17:34,505] INFO Checking MySql dialect for existence of TABLE "results" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:575)
[2021-12-08 12:17:34,512] INFO Using MySql dialect TABLE "results" present (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:583)
[2021-12-08 12:17:34,541] INFO Checking MySql dialect for type of TABLE "results" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:845)
[2021-12-08 12:17:34,548] INFO Setting metadata for table "results" to Table{name='"results"', type=TABLE columns=[Column{'expenses', isPrimaryKey=false, allowsNull=false, sqlType=DOUBLE}, Column{'profit', isPrimaryKey=false, allowsNull=false, sqlType=DOUBLE}, Column{'revenue', isPrimaryKey=false, allowsNull=false, sqlType=DOUBLE}]} (io.confluent.connect.jdbc.util.TableDefinitions:64)
[2021-12-08 12:17:38,940] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:17:39,049] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:17:48,946] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:17:49,056] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:17:58,962] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:17:59,057] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:18:08,973] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:18:09,066] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:18:18,983] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:18:19,077] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:18:28,990] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:18:29,083] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:18:38,995] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:18:39,089] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:18:49,012] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:18:49,090] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:18:59,027] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:18:59,103] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:19:09,031] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:19:09,108] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:19:19,032] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:19:19,110] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:19:29,035] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:19:29,114] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:19:39,043] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:19:39,119] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:19:49,051] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:19:49,129] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:19:59,064] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:19:59,141] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:20:09,073] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:20:09,150] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:20:19,086] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:20:19,163] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:20:29,097] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:20:29,177] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:20:39,107] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:20:39,184] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:20:49,112] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:20:49,193] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:20:59,118] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:20:59,197] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:21:09,122] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:21:09,201] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:21:19,135] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:21:19,211] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:21:29,142] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:21:29,222] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:21:39,145] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:21:39,236] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:21:49,154] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:21:49,246] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:21:59,160] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:21:59,253] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:22:09,161] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:22:09,268] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:22:19,168] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:22:19,277] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:22:29,176] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:22:29,286] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:22:39,189] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:22:39,297] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:22:49,198] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:22:49,310] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:22:59,200] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:22:59,324] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:23:09,201] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:23:09,339] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:23:19,209] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:23:19,347] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:23:29,215] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:23:29,355] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:23:39,218] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:23:39,356] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:23:49,233] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:23:49,372] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:23:59,240] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:23:59,379] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:24:09,251] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:24:09,390] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:24:19,259] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:24:19,400] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:24:29,275] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:24:29,415] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:24:39,292] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:24:39,419] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:24:49,304] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:24:49,427] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:24:59,314] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:24:59,436] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:25:09,319] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:25:09,447] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:25:19,325] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:25:19,450] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:25:29,337] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:25:29,460] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:25:39,342] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:25:39,466] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:25:49,351] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:25:49,474] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:25:59,363] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:25:59,488] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:26:09,368] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:26:09,490] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:26:19,379] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:26:19,504] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:26:29,385] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:26:29,509] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:26:39,396] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:26:39,520] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:26:49,398] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:26:49,521] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:26:59,407] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:26:59,531] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:27:09,408] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:27:09,545] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:27:19,419] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:27:19,555] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:27:29,422] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:27:29,564] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:27:39,426] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:27:39,566] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:27:49,441] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:27:49,579] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:27:59,457] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:27:59,595] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:28:09,471] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:28:09,596] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:28:19,478] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:28:19,602] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:28:29,489] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:28:29,613] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:28:39,496] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:28:39,621] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:28:49,504] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:28:49,629] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:28:59,506] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:28:59,631] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:29:09,515] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:29:09,639] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:29:19,529] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:29:19,652] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:29:29,545] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:29:29,655] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:29:39,553] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:29:39,663] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:29:49,557] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:29:49,667] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:29:59,571] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:29:59,681] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:30:09,574] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:30:09,682] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:30:19,589] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:30:19,696] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:30:29,591] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:30:29,699] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:30:39,607] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:30:39,713] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:30:49,612] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:30:49,722] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:30:59,619] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:30:59,728] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:31:09,621] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:31:09,730] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:31:19,630] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:31:19,739] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:31:29,640] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:31:29,750] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:31:39,645] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:31:39,756] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:31:49,657] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:31:49,765] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:31:59,665] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:31:59,776] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:32:09,678] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:32:09,785] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:32:19,685] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:32:19,794] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:32:29,694] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:32:29,802] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:32:39,707] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:32:39,817] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:32:49,708] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:32:49,831] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:32:59,712] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:32:59,838] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:33:09,717] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:33:09,855] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:33:19,732] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:33:19,857] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:33:29,733] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:33:29,858] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:33:39,744] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:33:39,867] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:33:49,755] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:33:49,868] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:33:59,772] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:33:59,880] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:34:09,785] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:34:09,894] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:34:19,792] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:34:19,900] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:34:29,809] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:34:29,916] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:34:39,810] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:34:39,919] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:34:49,827] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:34:49,923] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:34:59,832] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:34:59,941] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:35:09,843] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:35:09,951] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:35:19,847] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:35:19,957] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:35:29,857] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:35:29,967] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:35:39,860] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:35:39,969] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:35:49,869] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:35:49,978] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:35:59,881] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:35:59,991] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:36:09,886] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:36:09,994] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:36:19,887] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:36:19,995] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:36:29,893] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:36:30,002] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:36:39,896] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:36:40,005] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:36:49,902] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:36:50,011] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:36:59,908] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:37:00,015] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:37:09,920] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:37:10,016] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:37:19,938] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:37:20,030] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:37:29,944] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:37:30,036] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:37:39,957] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:37:40,051] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:37:49,961] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:37:50,055] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:37:59,966] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:38:00,058] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:38:09,972] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:38:10,064] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:38:19,984] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:38:20,078] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:38:29,997] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:38:30,090] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:38:40,003] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:38:40,097] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:38:50,007] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:38:50,100] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:39:00,018] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:39:00,112] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:39:10,034] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:39:10,128] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:39:20,039] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:39:20,133] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:39:30,043] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:39:30,135] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:39:40,044] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:39:40,137] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:39:50,052] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:39:50,146] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:40:00,064] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:40:00,155] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:40:10,079] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:40:10,158] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:40:20,089] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:40:20,168] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:40:30,092] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:40:30,184] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:40:40,101] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:40:40,193] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:40:50,113] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:40:50,206] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:41:00,117] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:41:00,212] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:41:10,128] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:41:10,220] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:41:20,143] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:41:20,235] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:41:30,147] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:41:30,241] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:41:40,154] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:41:40,247] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:41:50,159] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:41:50,252] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:42:00,167] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:42:00,259] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:42:10,169] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:42:10,262] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:42:20,183] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:42:20,276] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:42:30,189] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:42:30,284] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:42:40,192] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:42:40,284] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:42:50,196] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:42:50,291] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:43:00,201] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:43:00,296] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:43:10,210] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:43:10,302] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:43:18,640] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2021-12-08 12:43:18,641] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:327)
[2021-12-08 12:43:18,658] INFO Stopped http_8083@91c4a3f{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:381)
[2021-12-08 12:43:18,658] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:149)
[2021-12-08 12:43:18,691] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:344)
[2021-12-08 12:43:18,698] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:102)
[2021-12-08 12:43:18,708] INFO Stopping task jdbc-source-clients-0 (org.apache.kafka.connect.runtime.Worker:830)
[2021-12-08 12:43:18,710] INFO Stopping connector jdbc-source-clients (org.apache.kafka.connect.runtime.Worker:382)
[2021-12-08 12:43:18,710] INFO Scheduled shutdown for WorkerConnector{id=jdbc-source-clients} (org.apache.kafka.connect.runtime.WorkerConnector:248)
[2021-12-08 12:43:18,712] INFO Stopping table monitoring thread (io.confluent.connect.jdbc.JdbcSourceConnector:176)
[2021-12-08 12:43:18,713] INFO Shutting down thread monitoring tables. (io.confluent.connect.jdbc.source.TableMonitorThread:134)
[2021-12-08 12:43:18,723] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2021-12-08 12:43:18,731] INFO Completed shutdown for WorkerConnector{id=jdbc-source-clients} (org.apache.kafka.connect.runtime.WorkerConnector:268)
[2021-12-08 12:43:18,734] INFO Stopping task jdbc-source-managers-0 (org.apache.kafka.connect.runtime.Worker:830)
[2021-12-08 12:43:18,734] INFO Stopping connector jdbc-source-managers (org.apache.kafka.connect.runtime.Worker:382)
[2021-12-08 12:43:18,735] INFO Scheduled shutdown for WorkerConnector{id=jdbc-source-managers} (org.apache.kafka.connect.runtime.WorkerConnector:248)
[2021-12-08 12:43:18,735] INFO Stopping table monitoring thread (io.confluent.connect.jdbc.JdbcSourceConnector:176)
[2021-12-08 12:43:18,736] INFO Shutting down thread monitoring tables. (io.confluent.connect.jdbc.source.TableMonitorThread:134)
[2021-12-08 12:43:18,736] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2021-12-08 12:43:18,737] INFO Completed shutdown for WorkerConnector{id=jdbc-source-managers} (org.apache.kafka.connect.runtime.WorkerConnector:268)
[2021-12-08 12:43:18,738] INFO Stopping task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:830)
[2021-12-08 12:43:18,738] INFO Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:161)
[2021-12-08 12:43:18,739] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2021-12-08 12:43:18,740] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Revoke previously assigned partitions results-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2021-12-08 12:43:18,741] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Member connector-consumer-jdbc-sink-0-6fee5263-1355-490f-b84c-f1bbf6439dc0 sending LeaveGroup request to coordinator Nuno-PC:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1045)
[2021-12-08 12:43:18,782] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2021-12-08 12:43:18,783] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2021-12-08 12:43:18,789] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2021-12-08 12:43:18,816] INFO App info kafka.consumer for connector-consumer-jdbc-sink-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-12-08 12:43:18,823] INFO Stopping connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:382)
[2021-12-08 12:43:18,824] INFO Scheduled shutdown for WorkerConnector{id=jdbc-sink} (org.apache.kafka.connect.runtime.WorkerConnector:248)
[2021-12-08 12:43:18,832] INFO Completed shutdown for WorkerConnector{id=jdbc-sink} (org.apache.kafka.connect.runtime.WorkerConnector:268)
[2021-12-08 12:43:18,833] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:205)
[2021-12-08 12:43:18,834] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:66)
[2021-12-08 12:43:18,834] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2021-12-08 12:43:18,835] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2021-12-08 12:43:18,835] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2021-12-08 12:43:18,836] INFO App info kafka.connect for 169.254.211.170:8083 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-12-08 12:43:18,836] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:226)
[2021-12-08 12:43:18,839] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:120)
[2021-12-08 12:43:18,842] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
[2021-12-08 12:45:29,349] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:68)
[2021-12-08 12:45:29,360] INFO WorkerInfo values: 
	jvm.args = -Xmx256M, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=C:\kafka/logs, -Dlog4j.configuration=file:C:\kafka/config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 17.0.1, 17.0.1+12-LTS-39
	jvm.classpath = C:\kafka\libs\activation-1.1.1.jar;C:\kafka\libs\aopalliance-repackaged-2.6.1.jar;C:\kafka\libs\argparse4j-0.7.0.jar;C:\kafka\libs\audience-annotations-0.5.0.jar;C:\kafka\libs\commons-cli-1.4.jar;C:\kafka\libs\commons-lang3-3.8.1.jar;C:\kafka\libs\connect-api-2.8.1.jar;C:\kafka\libs\connect-basic-auth-extension-2.8.1.jar;C:\kafka\libs\connect-file-2.8.1.jar;C:\kafka\libs\connect-json-2.8.1.jar;C:\kafka\libs\connect-mirror-2.8.1.jar;C:\kafka\libs\connect-mirror-client-2.8.1.jar;C:\kafka\libs\connect-runtime-2.8.1.jar;C:\kafka\libs\connect-transforms-2.8.1.jar;C:\kafka\libs\hk2-api-2.6.1.jar;C:\kafka\libs\hk2-locator-2.6.1.jar;C:\kafka\libs\hk2-utils-2.6.1.jar;C:\kafka\libs\jackson-annotations-2.10.5.jar;C:\kafka\libs\jackson-core-2.10.5.jar;C:\kafka\libs\jackson-databind-2.10.5.1.jar;C:\kafka\libs\jackson-dataformat-csv-2.10.5.jar;C:\kafka\libs\jackson-datatype-jdk8-2.10.5.jar;C:\kafka\libs\jackson-jaxrs-base-2.10.5.jar;C:\kafka\libs\jackson-jaxrs-json-provider-2.10.5.jar;C:\kafka\libs\jackson-module-jaxb-annotations-2.10.5.jar;C:\kafka\libs\jackson-module-paranamer-2.10.5.jar;C:\kafka\libs\jackson-module-scala_2.12-2.10.5.jar;C:\kafka\libs\jakarta.activation-api-1.2.1.jar;C:\kafka\libs\jakarta.annotation-api-1.3.5.jar;C:\kafka\libs\jakarta.inject-2.6.1.jar;C:\kafka\libs\jakarta.validation-api-2.0.2.jar;C:\kafka\libs\jakarta.ws.rs-api-2.1.6.jar;C:\kafka\libs\jakarta.xml.bind-api-2.3.2.jar;C:\kafka\libs\javassist-3.27.0-GA.jar;C:\kafka\libs\javax.servlet-api-3.1.0.jar;C:\kafka\libs\javax.ws.rs-api-2.1.1.jar;C:\kafka\libs\jaxb-api-2.3.0.jar;C:\kafka\libs\jersey-client-2.34.jar;C:\kafka\libs\jersey-common-2.34.jar;C:\kafka\libs\jersey-container-servlet-2.34.jar;C:\kafka\libs\jersey-container-servlet-core-2.34.jar;C:\kafka\libs\jersey-hk2-2.34.jar;C:\kafka\libs\jersey-server-2.34.jar;C:\kafka\libs\jetty-client-9.4.43.v20210629.jar;C:\kafka\libs\jetty-continuation-9.4.43.v20210629.jar;C:\kafka\libs\jetty-http-9.4.43.v20210629.jar;C:\kafka\libs\jetty-io-9.4.43.v20210629.jar;C:\kafka\libs\jetty-security-9.4.43.v20210629.jar;C:\kafka\libs\jetty-server-9.4.43.v20210629.jar;C:\kafka\libs\jetty-servlet-9.4.43.v20210629.jar;C:\kafka\libs\jetty-servlets-9.4.43.v20210629.jar;C:\kafka\libs\jetty-util-9.4.43.v20210629.jar;C:\kafka\libs\jetty-util-ajax-9.4.43.v20210629.jar;C:\kafka\libs\jline-3.12.1.jar;C:\kafka\libs\jopt-simple-5.0.4.jar;C:\kafka\libs\kafka-clients-2.8.1.jar;C:\kafka\libs\kafka-connect-jdbc-10.2.5.jar;C:\kafka\libs\kafka-log4j-appender-2.8.1.jar;C:\kafka\libs\kafka-metadata-2.8.1.jar;C:\kafka\libs\kafka-raft-2.8.1.jar;C:\kafka\libs\kafka-shell-2.8.1.jar;C:\kafka\libs\kafka-streams-2.8.1.jar;C:\kafka\libs\kafka-streams-examples-2.8.1.jar;C:\kafka\libs\kafka-streams-scala_2.12-2.8.1.jar;C:\kafka\libs\kafka-streams-test-utils-2.8.1.jar;C:\kafka\libs\kafka-tools-2.8.1.jar;C:\kafka\libs\kafka_2.12-2.8.1-javadoc.jar;C:\kafka\libs\kafka_2.12-2.8.1-javadoc.jar.asc;C:\kafka\libs\kafka_2.12-2.8.1-sources.jar;C:\kafka\libs\kafka_2.12-2.8.1-sources.jar.asc;C:\kafka\libs\kafka_2.12-2.8.1-test-sources.jar;C:\kafka\libs\kafka_2.12-2.8.1-test-sources.jar.asc;C:\kafka\libs\kafka_2.12-2.8.1-test.jar;C:\kafka\libs\kafka_2.12-2.8.1-test.jar.asc;C:\kafka\libs\kafka_2.12-2.8.1.jar;C:\kafka\libs\kafka_2.12-2.8.1.jar.asc;C:\kafka\libs\log4j-1.2.17.jar;C:\kafka\libs\lz4-java-1.7.1.jar;C:\kafka\libs\maven-artifact-3.8.1.jar;C:\kafka\libs\metrics-core-2.2.0.jar;C:\kafka\libs\mysql-connector-java-8.0.27.jar;C:\kafka\libs\netty-buffer-4.1.62.Final.jar;C:\kafka\libs\netty-codec-4.1.62.Final.jar;C:\kafka\libs\netty-common-4.1.62.Final.jar;C:\kafka\libs\netty-handler-4.1.62.Final.jar;C:\kafka\libs\netty-resolver-4.1.62.Final.jar;C:\kafka\libs\netty-transport-4.1.62.Final.jar;C:\kafka\libs\netty-transport-native-epoll-4.1.62.Final.jar;C:\kafka\libs\netty-transport-native-unix-common-4.1.62.Final.jar;C:\kafka\libs\osgi-resource-locator-1.0.3.jar;C:\kafka\libs\paranamer-2.8.jar;C:\kafka\libs\plexus-utils-3.2.1.jar;C:\kafka\libs\reflections-0.9.12.jar;C:\kafka\libs\rocksdbjni-5.18.4.jar;C:\kafka\libs\scala-collection-compat_2.12-2.3.0.jar;C:\kafka\libs\scala-java8-compat_2.12-0.9.1.jar;C:\kafka\libs\scala-library-2.12.13.jar;C:\kafka\libs\scala-logging_2.12-3.9.2.jar;C:\kafka\libs\scala-reflect-2.12.13.jar;C:\kafka\libs\slf4j-api-1.7.30.jar;C:\kafka\libs\slf4j-log4j12-1.7.30.jar;C:\kafka\libs\snappy-java-1.1.8.1.jar;C:\kafka\libs\zookeeper-3.5.9.jar;C:\kafka\libs\zookeeper-jute-3.5.9.jar;C:\kafka\libs\zstd-jni-1.4.9-1.jar
	os.spec = Windows 10, amd64, 10.0
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2021-12-08 12:45:29,364] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectStandalone:77)
[2021-12-08 12:45:30,880] INFO Registered loader: jdk.internal.loader.ClassLoaders$AppClassLoader@266474c2 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2021-12-08 12:45:30,880] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,884] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,893] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,894] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,895] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,896] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,897] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,897] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,898] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,898] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,899] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,900] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,910] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,912] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,916] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,916] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,917] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,918] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,921] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,923] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,929] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,930] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,931] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,931] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,932] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,932] INFO Added plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,943] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,944] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,945] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,946] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,947] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,947] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,948] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,949] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,949] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,950] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,951] INFO Added plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,951] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,954] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,955] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,960] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,961] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,961] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,962] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,963] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,963] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,964] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,965] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,978] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,980] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,980] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,982] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:30,982] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:45:31,000] INFO Added aliases 'JdbcSinkConnector' and 'JdbcSink' to plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:45:31,000] INFO Added aliases 'JdbcSourceConnector' and 'JdbcSource' to plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:45:31,010] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:45:31,011] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:45:31,013] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:45:31,033] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:45:31,035] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:45:31,044] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:45:31,054] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:45:31,068] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:45:31,069] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:45:31,070] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:45:31,076] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:45:31,080] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:45:31,089] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:45:31,094] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:45:31,095] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:45:31,102] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:45:31,103] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:45:31,110] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:45:31,113] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:45:31,123] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:45:31,124] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:45:31,127] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:45:31,128] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:45:31,129] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:45:31,129] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:45:31,130] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:45:31,131] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-12-08 12:45:31,131] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:45:31,132] INFO Added aliases 'PredicatedTransformation' and 'Predicated' to plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:45:31,133] INFO Added alias 'Filter' to plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-12-08 12:45:31,134] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-12-08 12:45:31,134] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-12-08 12:45:31,135] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-12-08 12:45:31,146] INFO Added alias 'HasHeaderKey' to plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-12-08 12:45:31,152] INFO Added alias 'RecordIsTombstone' to plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-12-08 12:45:31,155] INFO Added alias 'TopicNameMatches' to plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-12-08 12:45:31,157] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-12-08 12:45:31,162] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:45:31,163] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:45:31,164] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:45:31,191] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	config.providers = []
	connector.client.config.override.policy = None
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	listeners = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = /tmp/connect.offsets
	plugin.path = null
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = null
	rest.port = 8083
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:372)
[2021-12-08 12:45:31,194] WARN Variables cannot be used in the 'plugin.path' property, since the property is used by plugin scanning before the config providers that replace the variables are initialized. The raw value 'null' was used for plugin scanning, as opposed to the transformed value 'null', and this may cause unexpected results. (org.apache.kafka.connect.runtime.WorkerConfig:420)
[2021-12-08 12:45:31,195] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2021-12-08 12:45:31,200] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:372)
[2021-12-08 12:45:31,362] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:45:31,363] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:45:31,366] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:45:31,374] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:45:31,374] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:45:31,375] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:45:31,376] INFO Kafka version: 2.8.1 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-12-08 12:45:31,376] INFO Kafka commitId: 839b886f9b732b15 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-12-08 12:45:31,377] INFO Kafka startTimeMs: 1638967531375 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-12-08 12:45:31,774] INFO Kafka cluster ID: u7MsxwDfR5Ob7FBWXv2wvA (org.apache.kafka.connect.util.ConnectUtils:65)
[2021-12-08 12:45:31,775] INFO App info kafka.admin.client for adminclient-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-12-08 12:45:31,787] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2021-12-08 12:45:31,787] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2021-12-08 12:45:31,788] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2021-12-08 12:45:31,800] INFO Logging initialized @2963ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:170)
[2021-12-08 12:45:31,861] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:132)
[2021-12-08 12:45:31,862] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:204)
[2021-12-08 12:45:31,873] INFO jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 17.0.1+12-LTS-39 (org.eclipse.jetty.server.Server:375)
[2021-12-08 12:45:31,912] INFO Started http_8083@3af9aa66{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:331)
[2021-12-08 12:45:31,912] INFO Started @3076ms (org.eclipse.jetty.server.Server:415)
[2021-12-08 12:45:31,946] INFO Advertised URI: http://169.254.211.170:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:371)
[2021-12-08 12:45:31,948] INFO REST server listening at http://169.254.211.170:8083/, advertising URL http://169.254.211.170:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2021-12-08 12:45:31,953] INFO Advertised URI: http://169.254.211.170:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:371)
[2021-12-08 12:45:31,966] INFO REST admin endpoints at http://169.254.211.170:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:220)
[2021-12-08 12:45:31,967] INFO Advertised URI: http://169.254.211.170:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:371)
[2021-12-08 12:45:31,967] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy:45)
[2021-12-08 12:45:31,982] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2021-12-08 12:45:31,983] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:372)
[2021-12-08 12:45:31,997] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:45:31,997] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:45:31,998] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:45:31,999] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:45:32,000] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:45:32,000] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:45:32,012] INFO Kafka version: 2.8.1 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-12-08 12:45:32,012] INFO Kafka commitId: 839b886f9b732b15 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-12-08 12:45:32,016] INFO Kafka startTimeMs: 1638967532012 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-12-08 12:45:32,050] INFO Kafka cluster ID: u7MsxwDfR5Ob7FBWXv2wvA (org.apache.kafka.connect.util.ConnectUtils:65)
[2021-12-08 12:45:32,055] INFO App info kafka.admin.client for adminclient-2 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-12-08 12:45:32,067] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2021-12-08 12:45:32,074] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2021-12-08 12:45:32,074] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2021-12-08 12:45:32,078] INFO Kafka version: 2.8.1 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-12-08 12:45:32,080] INFO Kafka commitId: 839b886f9b732b15 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-12-08 12:45:32,081] INFO Kafka startTimeMs: 1638967532078 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-12-08 12:45:32,223] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2021-12-08 12:45:32,226] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2021-12-08 12:45:32,249] INFO Kafka Connect standalone worker initialization took 2898ms (org.apache.kafka.connect.cli.ConnectStandalone:99)
[2021-12-08 12:45:32,250] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2021-12-08 12:45:32,257] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:94)
[2021-12-08 12:45:32,261] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:191)
[2021-12-08 12:45:32,263] INFO Starting FileOffsetBackingStore with file \tmp\connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:58)
[2021-12-08 12:45:32,275] INFO Worker started (org.apache.kafka.connect.runtime.Worker:198)
[2021-12-08 12:45:32,275] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:97)
[2021-12-08 12:45:32,276] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:224)
[2021-12-08 12:45:32,333] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:241)
[2021-12-08 12:45:32,403] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:334)
[2021-12-08 12:45:32,403] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:339)
[2021-12-08 12:45:32,406] INFO node0 Scavenging every 660000ms (org.eclipse.jetty.server.session:132)
[2021-12-08 12:45:32,927] INFO Started o.e.j.s.ServletContextHandler@4d774249{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:915)
[2021-12-08 12:45:32,928] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:319)
[2021-12-08 12:45:32,935] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2021-12-08 12:45:32,978] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:372)
[2021-12-08 12:45:33,004] INFO Creating connector jdbc-source-clients of type io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:271)
[2021-12-08 12:45:33,007] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-clients
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:372)
[2021-12-08 12:45:33,009] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-clients
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:45:33,015] INFO Instantiated connector jdbc-source-clients with version 10.2.5 of type class io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:281)
[2021-12-08 12:45:33,016] INFO Finished creating connector jdbc-source-clients (org.apache.kafka.connect.runtime.Worker:306)
[2021-12-08 12:45:33,020] INFO Starting JDBC Source Connector (io.confluent.connect.jdbc.JdbcSourceConnector:69)
[2021-12-08 12:45:33,030] INFO JdbcSourceConnectorConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:mysql://localhost:3306/project3
	connection.user = projeto
	db.timezone = UTC
	dialect.name = MySqlDatabaseDialect
	incrementing.column.name = 
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 60000
	query = SELECT * FROM clients;
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = []
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = []
	timestamp.delay.interval.ms = 0
	timestamp.initial = null
	topic.prefix = DBinfo
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceConnectorConfig:372)
[2021-12-08 12:45:33,043] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2021-12-08 12:45:33,225] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-clients
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:372)
[2021-12-08 12:45:33,226] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-clients
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:45:33,235] INFO Creating task jdbc-source-clients-0 (org.apache.kafka.connect.runtime.Worker:505)
[2021-12-08 12:45:33,240] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-clients
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:372)
[2021-12-08 12:45:33,241] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-clients
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:45:33,243] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.source.JdbcSourceTask
 (org.apache.kafka.connect.runtime.TaskConfig:372)
[2021-12-08 12:45:33,244] INFO Instantiated task jdbc-source-clients-0 with version 10.2.5 of type io.confluent.connect.jdbc.source.JdbcSourceTask (org.apache.kafka.connect.runtime.Worker:520)
[2021-12-08 12:45:33,245] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2021-12-08 12:45:33,245] INFO Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-source-clients-0 using the worker config (org.apache.kafka.connect.runtime.Worker:533)
[2021-12-08 12:45:33,246] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2021-12-08 12:45:33,246] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-source-clients-0 using the worker config (org.apache.kafka.connect.runtime.Worker:539)
[2021-12-08 12:45:33,247] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-source-clients-0 using the worker config (org.apache.kafka.connect.runtime.Worker:546)
[2021-12-08 12:45:33,253] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-clients
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:372)
[2021-12-08 12:45:33,265] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-clients
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:45:33,269] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:600)
[2021-12-08 12:45:33,275] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-jdbc-source-clients-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:372)
[2021-12-08 12:45:33,317] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-12-08 12:45:33,320] INFO Kafka version: 2.8.1 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-12-08 12:45:33,331] INFO Kafka commitId: 839b886f9b732b15 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-12-08 12:45:33,341] INFO Kafka startTimeMs: 1638967533320 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-12-08 12:45:33,359] INFO [Producer clientId=connector-producer-jdbc-source-clients-0] Cluster ID: u7MsxwDfR5Ob7FBWXv2wvA (org.apache.kafka.clients.Metadata:279)
[2021-12-08 12:45:33,376] INFO Starting JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:88)
[2021-12-08 12:45:33,380] INFO JdbcSourceTaskConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:mysql://localhost:3306/project3
	connection.user = projeto
	db.timezone = UTC
	dialect.name = MySqlDatabaseDialect
	incrementing.column.name = 
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 60000
	query = SELECT * FROM clients;
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = []
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	tables = []
	timestamp.column.name = []
	timestamp.delay.interval.ms = 0
	timestamp.initial = null
	topic.prefix = DBinfo
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceTaskConfig:372)
[2021-12-08 12:45:33,381] INFO Created connector jdbc-source-clients (org.apache.kafka.connect.cli.ConnectStandalone:109)
[2021-12-08 12:45:33,384] INFO Using JDBC dialect MySql (io.confluent.connect.jdbc.source.JdbcSourceTask:105)
[2021-12-08 12:45:33,397] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:372)
[2021-12-08 12:45:33,401] INFO Started JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:264)
[2021-12-08 12:45:33,409] INFO Creating connector jdbc-source-managers of type io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:271)
[2021-12-08 12:45:33,410] INFO WorkerSourceTask{id=jdbc-source-clients-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:232)
[2021-12-08 12:45:33,412] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-managers
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:372)
[2021-12-08 12:45:33,413] INFO WorkerSourceTask{id=jdbc-source-clients-0} Executing source task (org.apache.kafka.connect.runtime.WorkerSourceTask:238)
[2021-12-08 12:45:33,414] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-managers
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:45:33,415] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2021-12-08 12:45:33,416] INFO Instantiated connector jdbc-source-managers with version 10.2.5 of type class io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:281)
[2021-12-08 12:45:33,417] INFO Finished creating connector jdbc-source-managers (org.apache.kafka.connect.runtime.Worker:306)
[2021-12-08 12:45:33,418] INFO Starting JDBC Source Connector (io.confluent.connect.jdbc.JdbcSourceConnector:69)
[2021-12-08 12:45:33,421] INFO JdbcSourceConnectorConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:mysql://localhost:3306/project3
	connection.user = projeto
	db.timezone = UTC
	dialect.name = MySqlDatabaseDialect
	incrementing.column.name = 
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 60000
	query = SELECT * FROM managers;
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = []
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = []
	timestamp.delay.interval.ms = 0
	timestamp.initial = null
	topic.prefix = DBinfo
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceConnectorConfig:372)
[2021-12-08 12:45:33,428] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2021-12-08 12:45:33,443] INFO Begin using SQL query: SELECT * FROM clients; (io.confluent.connect.jdbc.source.TableQuerier:164)
[2021-12-08 12:45:33,448] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-managers
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:372)
[2021-12-08 12:45:33,449] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-managers
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:45:33,451] INFO Creating task jdbc-source-managers-0 (org.apache.kafka.connect.runtime.Worker:505)
[2021-12-08 12:45:33,453] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-managers
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:372)
[2021-12-08 12:45:33,455] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-managers
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:45:33,468] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.source.JdbcSourceTask
 (org.apache.kafka.connect.runtime.TaskConfig:372)
[2021-12-08 12:45:33,475] INFO Instantiated task jdbc-source-managers-0 with version 10.2.5 of type io.confluent.connect.jdbc.source.JdbcSourceTask (org.apache.kafka.connect.runtime.Worker:520)
[2021-12-08 12:45:33,476] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2021-12-08 12:45:33,477] INFO Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-source-managers-0 using the worker config (org.apache.kafka.connect.runtime.Worker:533)
[2021-12-08 12:45:33,478] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2021-12-08 12:45:33,478] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-source-managers-0 using the worker config (org.apache.kafka.connect.runtime.Worker:539)
[2021-12-08 12:45:33,479] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-source-managers-0 using the worker config (org.apache.kafka.connect.runtime.Worker:546)
[2021-12-08 12:45:33,480] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-managers
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:372)
[2021-12-08 12:45:33,481] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-managers
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:45:33,482] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:600)
[2021-12-08 12:45:33,484] ERROR Non-transient SQL exception while running query for table: BulkTableQuerier{table='null', query='SELECT * FROM clients;', topicPrefix='DBinfo'} (io.confluent.connect.jdbc.source.JdbcSourceTask:412)
java.sql.SQLSyntaxErrorException: Table 'project3.clients' doesn't exist
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeQuery(ClientPreparedStatement.java:1009)
	at io.confluent.connect.jdbc.source.BulkTableQuerier.executeQuery(BulkTableQuerier.java:79)
	at io.confluent.connect.jdbc.source.TableQuerier.maybeStartQuery(TableQuerier.java:98)
	at io.confluent.connect.jdbc.source.BulkTableQuerier.maybeStartQuery(BulkTableQuerier.java:39)
	at io.confluent.connect.jdbc.source.JdbcSourceTask.poll(JdbcSourceTask.java:379)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.poll(WorkerSourceTask.java:296)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.execute(WorkerSourceTask.java:253)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
[2021-12-08 12:45:33,484] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-jdbc-source-managers-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:372)
[2021-12-08 12:45:33,501] INFO Closing resources for JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:332)
[2021-12-08 12:45:33,505] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2021-12-08 12:45:33,507] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:45:33,507] ERROR WorkerSourceTask{id=jdbc-source-clients-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:190)
org.apache.kafka.connect.errors.ConnectException: java.sql.SQLSyntaxErrorException: Table 'project3.clients' doesn't exist
	at io.confluent.connect.jdbc.source.JdbcSourceTask.poll(JdbcSourceTask.java:417)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.poll(WorkerSourceTask.java:296)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.execute(WorkerSourceTask.java:253)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLSyntaxErrorException: Table 'project3.clients' doesn't exist
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeQuery(ClientPreparedStatement.java:1009)
	at io.confluent.connect.jdbc.source.BulkTableQuerier.executeQuery(BulkTableQuerier.java:79)
	at io.confluent.connect.jdbc.source.TableQuerier.maybeStartQuery(TableQuerier.java:98)
	at io.confluent.connect.jdbc.source.BulkTableQuerier.maybeStartQuery(BulkTableQuerier.java:39)
	at io.confluent.connect.jdbc.source.JdbcSourceTask.poll(JdbcSourceTask.java:379)
	... 9 more
[2021-12-08 12:45:33,509] INFO Stopping JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:320)
[2021-12-08 12:45:33,509] INFO Closing resources for JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:332)
[2021-12-08 12:45:33,510] INFO [Producer clientId=connector-producer-jdbc-source-clients-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1204)
[2021-12-08 12:45:33,511] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-12-08 12:45:33,512] INFO Kafka version: 2.8.1 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-12-08 12:45:33,513] INFO Kafka commitId: 839b886f9b732b15 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-12-08 12:45:33,513] INFO Kafka startTimeMs: 1638967533512 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-12-08 12:45:33,514] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2021-12-08 12:45:33,515] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2021-12-08 12:45:33,529] INFO [Producer clientId=connector-producer-jdbc-source-managers-0] Cluster ID: u7MsxwDfR5Ob7FBWXv2wvA (org.apache.kafka.clients.Metadata:279)
[2021-12-08 12:45:33,517] INFO Created connector jdbc-source-managers (org.apache.kafka.connect.cli.ConnectStandalone:109)
[2021-12-08 12:45:33,533] INFO Starting JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:88)
[2021-12-08 12:45:33,532] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2021-12-08 12:45:33,543] INFO JdbcSourceTaskConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:mysql://localhost:3306/project3
	connection.user = projeto
	db.timezone = UTC
	dialect.name = MySqlDatabaseDialect
	incrementing.column.name = 
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 60000
	query = SELECT * FROM managers;
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = []
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	tables = []
	timestamp.column.name = []
	timestamp.delay.interval.ms = 0
	timestamp.initial = null
	topic.prefix = DBinfo
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceTaskConfig:372)
[2021-12-08 12:45:33,543] INFO App info kafka.producer for connector-producer-jdbc-source-clients-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-12-08 12:45:33,545] INFO Using JDBC dialect MySql (io.confluent.connect.jdbc.source.JdbcSourceTask:105)
[2021-12-08 12:45:33,546] INFO Started JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:264)
[2021-12-08 12:45:33,547] INFO WorkerSourceTask{id=jdbc-source-managers-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:232)
[2021-12-08 12:45:33,549] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:372)
[2021-12-08 12:45:33,558] INFO WorkerSourceTask{id=jdbc-source-managers-0} Executing source task (org.apache.kafka.connect.runtime.WorkerSourceTask:238)
[2021-12-08 12:45:33,561] INFO Creating connector jdbc-sink of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:271)
[2021-12-08 12:45:33,563] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2021-12-08 12:45:33,564] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	topics = [results]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2021-12-08 12:45:33,565] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	topics = [results]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:45:33,566] INFO Instantiated connector jdbc-sink with version 10.2.5 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:281)
[2021-12-08 12:45:33,567] INFO Finished creating connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:306)
[2021-12-08 12:45:33,570] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	topics = [results]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2021-12-08 12:45:33,572] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	topics = [results]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:45:33,578] INFO Begin using SQL query: SELECT * FROM managers; (io.confluent.connect.jdbc.source.TableQuerier:164)
[2021-12-08 12:45:33,578] INFO Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:44)
[2021-12-08 12:45:33,590] INFO Creating task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:505)
[2021-12-08 12:45:33,590] ERROR Non-transient SQL exception while running query for table: BulkTableQuerier{table='null', query='SELECT * FROM managers;', topicPrefix='DBinfo'} (io.confluent.connect.jdbc.source.JdbcSourceTask:412)
java.sql.SQLSyntaxErrorException: Table 'project3.managers' doesn't exist
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeQuery(ClientPreparedStatement.java:1009)
	at io.confluent.connect.jdbc.source.BulkTableQuerier.executeQuery(BulkTableQuerier.java:79)
	at io.confluent.connect.jdbc.source.TableQuerier.maybeStartQuery(TableQuerier.java:98)
	at io.confluent.connect.jdbc.source.BulkTableQuerier.maybeStartQuery(BulkTableQuerier.java:39)
	at io.confluent.connect.jdbc.source.JdbcSourceTask.poll(JdbcSourceTask.java:379)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.poll(WorkerSourceTask.java:296)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.execute(WorkerSourceTask.java:253)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
[2021-12-08 12:45:33,593] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:372)
[2021-12-08 12:45:33,594] INFO Closing resources for JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:332)
[2021-12-08 12:45:33,595] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:45:33,595] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2021-12-08 12:45:33,596] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:372)
[2021-12-08 12:45:33,597] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:45:33,597] INFO Instantiated task jdbc-sink-0 with version 10.2.5 of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:520)
[2021-12-08 12:45:33,598] ERROR WorkerSourceTask{id=jdbc-source-managers-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:190)
org.apache.kafka.connect.errors.ConnectException: java.sql.SQLSyntaxErrorException: Table 'project3.managers' doesn't exist
	at io.confluent.connect.jdbc.source.JdbcSourceTask.poll(JdbcSourceTask.java:417)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.poll(WorkerSourceTask.java:296)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.execute(WorkerSourceTask.java:253)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLSyntaxErrorException: Table 'project3.managers' doesn't exist
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeQuery(ClientPreparedStatement.java:1009)
	at io.confluent.connect.jdbc.source.BulkTableQuerier.executeQuery(BulkTableQuerier.java:79)
	at io.confluent.connect.jdbc.source.TableQuerier.maybeStartQuery(TableQuerier.java:98)
	at io.confluent.connect.jdbc.source.BulkTableQuerier.maybeStartQuery(BulkTableQuerier.java:39)
	at io.confluent.connect.jdbc.source.JdbcSourceTask.poll(JdbcSourceTask.java:379)
	... 9 more
[2021-12-08 12:45:33,600] INFO Stopping JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:320)
[2021-12-08 12:45:33,599] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:372)
[2021-12-08 12:45:33,601] INFO Closing resources for JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:332)
[2021-12-08 12:45:33,603] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2021-12-08 12:45:33,607] INFO [Producer clientId=connector-producer-jdbc-source-managers-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1204)
[2021-12-08 12:45:33,608] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-sink-0 using the connector config (org.apache.kafka.connect.runtime.Worker:535)
[2021-12-08 12:45:33,618] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2021-12-08 12:45:33,620] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-sink-0 using the connector config (org.apache.kafka.connect.runtime.Worker:541)
[2021-12-08 12:45:33,622] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2021-12-08 12:45:33,625] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2021-12-08 12:45:33,625] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:546)
[2021-12-08 12:45:33,626] INFO App info kafka.producer for connector-producer-jdbc-source-managers-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-12-08 12:45:33,627] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:626)
[2021-12-08 12:45:33,628] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	topics = [results]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2021-12-08 12:45:33,629] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	topics = [results]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:45:33,636] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-jdbc-sink-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-jdbc-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:372)
[2021-12-08 12:45:33,676] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-12-08 12:45:33,677] INFO Kafka version: 2.8.1 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-12-08 12:45:33,679] INFO Kafka commitId: 839b886f9b732b15 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-12-08 12:45:33,688] INFO Kafka startTimeMs: 1638967533677 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-12-08 12:45:33,692] INFO Created connector jdbc-sink (org.apache.kafka.connect.cli.ConnectStandalone:109)
[2021-12-08 12:45:33,693] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Subscribed to topic(s): results (org.apache.kafka.clients.consumer.KafkaConsumer:965)
[2021-12-08 12:45:33,694] INFO Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:48)
[2021-12-08 12:45:33,695] INFO JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = false
	batch.size = 3000
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:mysql://localhost:3306/project3
	connection.user = projeto
	db.timezone = UTC
	delete.enabled = false
	dialect.name = MySqlDatabaseDialect
	fields.whitelist = [profit, revenue]
	insert.mode = insert
	max.retries = 10
	pk.fields = []
	pk.mode = none
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = ${topic}
	table.types = [TABLE]
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:372)
[2021-12-08 12:45:33,697] INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:67)
[2021-12-08 12:45:33,698] INFO WorkerSinkTask{id=jdbc-sink-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:310)
[2021-12-08 12:45:33,699] INFO WorkerSinkTask{id=jdbc-sink-0} Executing sink task (org.apache.kafka.connect.runtime.WorkerSinkTask:196)
[2021-12-08 12:45:33,732] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Cluster ID: u7MsxwDfR5Ob7FBWXv2wvA (org.apache.kafka.clients.Metadata:279)
[2021-12-08 12:45:33,737] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Discovered group coordinator Nuno-PC:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:850)
[2021-12-08 12:45:33,747] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-12-08 12:45:33,786] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-12-08 12:45:33,816] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Successfully joined group with generation Generation{generationId=5, memberId='connector-consumer-jdbc-sink-0-92630dc4-d0aa-4355-9581-5686f06c2446', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-12-08 12:45:33,825] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Finished assignment for group at generation 5: {connector-consumer-jdbc-sink-0-92630dc4-d0aa-4355-9581-5686f06c2446=Assignment(partitions=[results-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:626)
[2021-12-08 12:45:33,921] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Successfully synced group in generation Generation{generationId=5, memberId='connector-consumer-jdbc-sink-0-92630dc4-d0aa-4355-9581-5686f06c2446', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:760)
[2021-12-08 12:45:33,922] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Notifying assignor about the new Assignment(partitions=[results-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-12-08 12:45:33,927] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Adding newly assigned partitions: results-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-12-08 12:45:33,952] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Setting offset for partition results-0 to the committed offset FetchPosition{offset=1, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[Nuno-PC:9092 (id: 0 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:820)
[2021-12-08 12:45:43,382] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:45:43,520] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:45:46,141] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2021-12-08 12:45:46,146] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2021-12-08 12:45:46,166] INFO Checking MySql dialect for existence of TABLE "results" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:575)
[2021-12-08 12:45:46,178] INFO Using MySql dialect TABLE "results" present (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:583)
[2021-12-08 12:45:46,217] INFO Checking MySql dialect for type of TABLE "results" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:845)
[2021-12-08 12:45:46,232] INFO Setting metadata for table "results" to Table{name='"results"', type=TABLE columns=[Column{'expenses', isPrimaryKey=false, allowsNull=false, sqlType=DOUBLE}, Column{'profit', isPrimaryKey=false, allowsNull=false, sqlType=DOUBLE}, Column{'revenue', isPrimaryKey=false, allowsNull=false, sqlType=DOUBLE}]} (io.confluent.connect.jdbc.util.TableDefinitions:64)
[2021-12-08 12:45:46,239] WARN Write of 1 records failed, remainingRetries=10 (io.confluent.connect.jdbc.sink.JdbcSinkTask:92)
java.sql.BatchUpdateException: Field 'expenses' doesn't have a default value
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)
	at com.mysql.cj.util.Util.handleNewInstance(Util.java:192)
	at com.mysql.cj.util.Util.getInstance(Util.java:167)
	at com.mysql.cj.util.Util.getInstance(Util.java:174)
	at com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:853)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:435)
	at com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:800)
	at io.confluent.connect.jdbc.sink.BufferedRecords.executeUpdates(BufferedRecords.java:221)
	at io.confluent.connect.jdbc.sink.BufferedRecords.flush(BufferedRecords.java:187)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:80)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:84)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:582)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:330)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLException: Field 'expenses' doesn't have a default value
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:129)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1098)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:832)
	... 17 more
[2021-12-08 12:45:46,243] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2021-12-08 12:45:46,248] INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:67)
[2021-12-08 12:45:46,248] ERROR WorkerSinkTask{id=jdbc-sink-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:601)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: Field 'expenses' doesn't have a default value
java.sql.SQLException: Field 'expenses' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:582)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:330)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: Field 'expenses' doesn't have a default value
java.sql.SQLException: Field 'expenses' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:150)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:102)
	... 11 more
[2021-12-08 12:45:49,262] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2021-12-08 12:45:49,265] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2021-12-08 12:45:49,272] INFO Checking MySql dialect for existence of TABLE "results" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:575)
[2021-12-08 12:45:49,279] INFO Using MySql dialect TABLE "results" present (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:583)
[2021-12-08 12:45:49,312] INFO Checking MySql dialect for type of TABLE "results" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:845)
[2021-12-08 12:45:49,324] INFO Setting metadata for table "results" to Table{name='"results"', type=TABLE columns=[Column{'expenses', isPrimaryKey=false, allowsNull=false, sqlType=DOUBLE}, Column{'profit', isPrimaryKey=false, allowsNull=false, sqlType=DOUBLE}, Column{'revenue', isPrimaryKey=false, allowsNull=false, sqlType=DOUBLE}]} (io.confluent.connect.jdbc.util.TableDefinitions:64)
[2021-12-08 12:45:49,327] WARN Write of 1 records failed, remainingRetries=9 (io.confluent.connect.jdbc.sink.JdbcSinkTask:92)
java.sql.BatchUpdateException: Field 'expenses' doesn't have a default value
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)
	at com.mysql.cj.util.Util.handleNewInstance(Util.java:192)
	at com.mysql.cj.util.Util.getInstance(Util.java:167)
	at com.mysql.cj.util.Util.getInstance(Util.java:174)
	at com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:853)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:435)
	at com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:800)
	at io.confluent.connect.jdbc.sink.BufferedRecords.executeUpdates(BufferedRecords.java:221)
	at io.confluent.connect.jdbc.sink.BufferedRecords.flush(BufferedRecords.java:187)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:80)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:84)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:582)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:330)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLException: Field 'expenses' doesn't have a default value
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:129)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1098)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:832)
	... 17 more
[2021-12-08 12:45:49,330] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2021-12-08 12:45:49,331] INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:67)
[2021-12-08 12:45:49,331] ERROR WorkerSinkTask{id=jdbc-sink-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:601)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: Field 'expenses' doesn't have a default value
java.sql.SQLException: Field 'expenses' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:582)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:330)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: Field 'expenses' doesn't have a default value
java.sql.SQLException: Field 'expenses' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:150)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:102)
	... 11 more
[2021-12-08 12:45:52,332] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2021-12-08 12:45:52,337] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2021-12-08 12:45:52,344] INFO Checking MySql dialect for existence of TABLE "results" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:575)
[2021-12-08 12:45:52,351] INFO Using MySql dialect TABLE "results" present (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:583)
[2021-12-08 12:45:52,397] INFO Checking MySql dialect for type of TABLE "results" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:845)
[2021-12-08 12:45:52,411] INFO Setting metadata for table "results" to Table{name='"results"', type=TABLE columns=[Column{'expenses', isPrimaryKey=false, allowsNull=false, sqlType=DOUBLE}, Column{'profit', isPrimaryKey=false, allowsNull=false, sqlType=DOUBLE}, Column{'revenue', isPrimaryKey=false, allowsNull=false, sqlType=DOUBLE}]} (io.confluent.connect.jdbc.util.TableDefinitions:64)
[2021-12-08 12:45:52,413] WARN Write of 1 records failed, remainingRetries=8 (io.confluent.connect.jdbc.sink.JdbcSinkTask:92)
java.sql.BatchUpdateException: Field 'expenses' doesn't have a default value
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)
	at com.mysql.cj.util.Util.handleNewInstance(Util.java:192)
	at com.mysql.cj.util.Util.getInstance(Util.java:167)
	at com.mysql.cj.util.Util.getInstance(Util.java:174)
	at com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:853)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:435)
	at com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:800)
	at io.confluent.connect.jdbc.sink.BufferedRecords.executeUpdates(BufferedRecords.java:221)
	at io.confluent.connect.jdbc.sink.BufferedRecords.flush(BufferedRecords.java:187)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:80)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:84)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:582)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:330)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLException: Field 'expenses' doesn't have a default value
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:129)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1098)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:832)
	... 17 more
[2021-12-08 12:45:52,414] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2021-12-08 12:45:52,416] INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:67)
[2021-12-08 12:45:52,417] ERROR WorkerSinkTask{id=jdbc-sink-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:601)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: Field 'expenses' doesn't have a default value
java.sql.SQLException: Field 'expenses' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:582)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:330)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: Field 'expenses' doesn't have a default value
java.sql.SQLException: Field 'expenses' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:150)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:102)
	... 11 more
[2021-12-08 12:45:53,388] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:45:53,527] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:45:53,702] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2021-12-08 12:45:53,706] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2021-12-08 12:45:53,712] INFO Checking MySql dialect for existence of TABLE "results" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:575)
[2021-12-08 12:45:53,726] INFO Using MySql dialect TABLE "results" present (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:583)
[2021-12-08 12:45:53,765] INFO Checking MySql dialect for type of TABLE "results" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:845)
[2021-12-08 12:45:53,777] INFO Setting metadata for table "results" to Table{name='"results"', type=TABLE columns=[Column{'expenses', isPrimaryKey=false, allowsNull=false, sqlType=DOUBLE}, Column{'profit', isPrimaryKey=false, allowsNull=false, sqlType=DOUBLE}, Column{'revenue', isPrimaryKey=false, allowsNull=false, sqlType=DOUBLE}]} (io.confluent.connect.jdbc.util.TableDefinitions:64)
[2021-12-08 12:45:53,780] WARN Write of 1 records failed, remainingRetries=7 (io.confluent.connect.jdbc.sink.JdbcSinkTask:92)
java.sql.BatchUpdateException: Field 'expenses' doesn't have a default value
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)
	at com.mysql.cj.util.Util.handleNewInstance(Util.java:192)
	at com.mysql.cj.util.Util.getInstance(Util.java:167)
	at com.mysql.cj.util.Util.getInstance(Util.java:174)
	at com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:853)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:435)
	at com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:800)
	at io.confluent.connect.jdbc.sink.BufferedRecords.executeUpdates(BufferedRecords.java:221)
	at io.confluent.connect.jdbc.sink.BufferedRecords.flush(BufferedRecords.java:187)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:80)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:84)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:582)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:330)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLException: Field 'expenses' doesn't have a default value
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:129)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1098)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:832)
	... 17 more
[2021-12-08 12:45:53,783] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2021-12-08 12:45:53,785] INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:67)
[2021-12-08 12:45:53,798] ERROR WorkerSinkTask{id=jdbc-sink-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:601)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: Field 'expenses' doesn't have a default value
java.sql.SQLException: Field 'expenses' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:582)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:330)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: Field 'expenses' doesn't have a default value
java.sql.SQLException: Field 'expenses' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:150)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:102)
	... 11 more
[2021-12-08 12:45:56,819] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2021-12-08 12:45:56,824] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2021-12-08 12:45:56,829] INFO Checking MySql dialect for existence of TABLE "results" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:575)
[2021-12-08 12:45:56,841] INFO Using MySql dialect TABLE "results" present (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:583)
[2021-12-08 12:45:56,876] INFO Checking MySql dialect for type of TABLE "results" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:845)
[2021-12-08 12:45:56,898] INFO Setting metadata for table "results" to Table{name='"results"', type=TABLE columns=[Column{'expenses', isPrimaryKey=false, allowsNull=false, sqlType=DOUBLE}, Column{'profit', isPrimaryKey=false, allowsNull=false, sqlType=DOUBLE}, Column{'revenue', isPrimaryKey=false, allowsNull=false, sqlType=DOUBLE}]} (io.confluent.connect.jdbc.util.TableDefinitions:64)
[2021-12-08 12:45:56,902] WARN Write of 1 records failed, remainingRetries=6 (io.confluent.connect.jdbc.sink.JdbcSinkTask:92)
java.sql.BatchUpdateException: Field 'expenses' doesn't have a default value
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)
	at com.mysql.cj.util.Util.handleNewInstance(Util.java:192)
	at com.mysql.cj.util.Util.getInstance(Util.java:167)
	at com.mysql.cj.util.Util.getInstance(Util.java:174)
	at com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:853)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:435)
	at com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:800)
	at io.confluent.connect.jdbc.sink.BufferedRecords.executeUpdates(BufferedRecords.java:221)
	at io.confluent.connect.jdbc.sink.BufferedRecords.flush(BufferedRecords.java:187)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:80)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:84)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:582)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:330)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLException: Field 'expenses' doesn't have a default value
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:129)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1098)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:832)
	... 17 more
[2021-12-08 12:45:56,904] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2021-12-08 12:45:56,907] INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:67)
[2021-12-08 12:45:56,907] ERROR WorkerSinkTask{id=jdbc-sink-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:601)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: Field 'expenses' doesn't have a default value
java.sql.SQLException: Field 'expenses' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:582)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:330)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: Field 'expenses' doesn't have a default value
java.sql.SQLException: Field 'expenses' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:150)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:102)
	... 11 more
[2021-12-08 12:45:59,924] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2021-12-08 12:45:59,929] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2021-12-08 12:45:59,934] INFO Checking MySql dialect for existence of TABLE "results" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:575)
[2021-12-08 12:45:59,942] INFO Using MySql dialect TABLE "results" present (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:583)
[2021-12-08 12:45:59,988] INFO Checking MySql dialect for type of TABLE "results" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:845)
[2021-12-08 12:46:00,001] INFO Setting metadata for table "results" to Table{name='"results"', type=TABLE columns=[Column{'expenses', isPrimaryKey=false, allowsNull=false, sqlType=DOUBLE}, Column{'profit', isPrimaryKey=false, allowsNull=false, sqlType=DOUBLE}, Column{'revenue', isPrimaryKey=false, allowsNull=false, sqlType=DOUBLE}]} (io.confluent.connect.jdbc.util.TableDefinitions:64)
[2021-12-08 12:46:00,003] WARN Write of 1 records failed, remainingRetries=5 (io.confluent.connect.jdbc.sink.JdbcSinkTask:92)
java.sql.BatchUpdateException: Field 'expenses' doesn't have a default value
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)
	at com.mysql.cj.util.Util.handleNewInstance(Util.java:192)
	at com.mysql.cj.util.Util.getInstance(Util.java:167)
	at com.mysql.cj.util.Util.getInstance(Util.java:174)
	at com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:853)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:435)
	at com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:800)
	at io.confluent.connect.jdbc.sink.BufferedRecords.executeUpdates(BufferedRecords.java:221)
	at io.confluent.connect.jdbc.sink.BufferedRecords.flush(BufferedRecords.java:187)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:80)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:84)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:582)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:330)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLException: Field 'expenses' doesn't have a default value
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:129)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1098)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:832)
	... 17 more
[2021-12-08 12:46:00,009] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2021-12-08 12:46:00,010] INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:67)
[2021-12-08 12:46:00,026] ERROR WorkerSinkTask{id=jdbc-sink-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:601)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: Field 'expenses' doesn't have a default value
java.sql.SQLException: Field 'expenses' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:582)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:330)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: Field 'expenses' doesn't have a default value
java.sql.SQLException: Field 'expenses' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:150)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:102)
	... 11 more
[2021-12-08 12:46:03,047] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2021-12-08 12:46:03,052] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2021-12-08 12:46:03,058] INFO Checking MySql dialect for existence of TABLE "results" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:575)
[2021-12-08 12:46:03,080] INFO Using MySql dialect TABLE "results" present (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:583)
[2021-12-08 12:46:03,135] INFO Checking MySql dialect for type of TABLE "results" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:845)
[2021-12-08 12:46:03,157] INFO Setting metadata for table "results" to Table{name='"results"', type=TABLE columns=[Column{'expenses', isPrimaryKey=false, allowsNull=false, sqlType=DOUBLE}, Column{'profit', isPrimaryKey=false, allowsNull=false, sqlType=DOUBLE}, Column{'revenue', isPrimaryKey=false, allowsNull=false, sqlType=DOUBLE}]} (io.confluent.connect.jdbc.util.TableDefinitions:64)
[2021-12-08 12:46:03,164] WARN Write of 1 records failed, remainingRetries=4 (io.confluent.connect.jdbc.sink.JdbcSinkTask:92)
java.sql.BatchUpdateException: Field 'expenses' doesn't have a default value
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)
	at com.mysql.cj.util.Util.handleNewInstance(Util.java:192)
	at com.mysql.cj.util.Util.getInstance(Util.java:167)
	at com.mysql.cj.util.Util.getInstance(Util.java:174)
	at com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:853)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:435)
	at com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:800)
	at io.confluent.connect.jdbc.sink.BufferedRecords.executeUpdates(BufferedRecords.java:221)
	at io.confluent.connect.jdbc.sink.BufferedRecords.flush(BufferedRecords.java:187)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:80)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:84)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:582)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:330)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLException: Field 'expenses' doesn't have a default value
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:129)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1098)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:832)
	... 17 more
[2021-12-08 12:46:03,166] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2021-12-08 12:46:03,168] INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:67)
[2021-12-08 12:46:03,178] ERROR WorkerSinkTask{id=jdbc-sink-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:601)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: Field 'expenses' doesn't have a default value
java.sql.SQLException: Field 'expenses' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:582)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:330)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: Field 'expenses' doesn't have a default value
java.sql.SQLException: Field 'expenses' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:150)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:102)
	... 11 more
[2021-12-08 12:46:03,404] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:46:03,543] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:46:03,825] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2021-12-08 12:46:03,829] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2021-12-08 12:46:03,835] INFO Checking MySql dialect for existence of TABLE "results" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:575)
[2021-12-08 12:46:03,841] INFO Using MySql dialect TABLE "results" present (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:583)
[2021-12-08 12:46:03,897] INFO Checking MySql dialect for type of TABLE "results" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:845)
[2021-12-08 12:46:03,912] INFO Setting metadata for table "results" to Table{name='"results"', type=TABLE columns=[Column{'expenses', isPrimaryKey=false, allowsNull=false, sqlType=DOUBLE}, Column{'profit', isPrimaryKey=false, allowsNull=false, sqlType=DOUBLE}, Column{'revenue', isPrimaryKey=false, allowsNull=false, sqlType=DOUBLE}]} (io.confluent.connect.jdbc.util.TableDefinitions:64)
[2021-12-08 12:46:03,915] WARN Write of 1 records failed, remainingRetries=3 (io.confluent.connect.jdbc.sink.JdbcSinkTask:92)
java.sql.BatchUpdateException: Field 'expenses' doesn't have a default value
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)
	at com.mysql.cj.util.Util.handleNewInstance(Util.java:192)
	at com.mysql.cj.util.Util.getInstance(Util.java:167)
	at com.mysql.cj.util.Util.getInstance(Util.java:174)
	at com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:853)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:435)
	at com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:800)
	at io.confluent.connect.jdbc.sink.BufferedRecords.executeUpdates(BufferedRecords.java:221)
	at io.confluent.connect.jdbc.sink.BufferedRecords.flush(BufferedRecords.java:187)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:80)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:84)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:582)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:330)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLException: Field 'expenses' doesn't have a default value
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:129)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1098)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:832)
	... 17 more
[2021-12-08 12:46:03,922] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2021-12-08 12:46:03,934] INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:67)
[2021-12-08 12:46:03,943] ERROR WorkerSinkTask{id=jdbc-sink-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:601)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: Field 'expenses' doesn't have a default value
java.sql.SQLException: Field 'expenses' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:582)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:330)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: Field 'expenses' doesn't have a default value
java.sql.SQLException: Field 'expenses' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:150)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:102)
	... 11 more
[2021-12-08 12:46:06,970] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2021-12-08 12:46:06,974] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2021-12-08 12:46:06,984] INFO Checking MySql dialect for existence of TABLE "results" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:575)
[2021-12-08 12:46:06,993] INFO Using MySql dialect TABLE "results" present (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:583)
[2021-12-08 12:46:07,033] INFO Checking MySql dialect for type of TABLE "results" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:845)
[2021-12-08 12:46:07,041] INFO Setting metadata for table "results" to Table{name='"results"', type=TABLE columns=[Column{'expenses', isPrimaryKey=false, allowsNull=false, sqlType=DOUBLE}, Column{'profit', isPrimaryKey=false, allowsNull=false, sqlType=DOUBLE}, Column{'revenue', isPrimaryKey=false, allowsNull=false, sqlType=DOUBLE}]} (io.confluent.connect.jdbc.util.TableDefinitions:64)
[2021-12-08 12:46:07,043] WARN Write of 1 records failed, remainingRetries=2 (io.confluent.connect.jdbc.sink.JdbcSinkTask:92)
java.sql.BatchUpdateException: Field 'expenses' doesn't have a default value
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)
	at com.mysql.cj.util.Util.handleNewInstance(Util.java:192)
	at com.mysql.cj.util.Util.getInstance(Util.java:167)
	at com.mysql.cj.util.Util.getInstance(Util.java:174)
	at com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:853)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:435)
	at com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:800)
	at io.confluent.connect.jdbc.sink.BufferedRecords.executeUpdates(BufferedRecords.java:221)
	at io.confluent.connect.jdbc.sink.BufferedRecords.flush(BufferedRecords.java:187)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:80)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:84)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:582)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:330)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLException: Field 'expenses' doesn't have a default value
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:129)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1098)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:832)
	... 17 more
[2021-12-08 12:46:07,045] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2021-12-08 12:46:07,046] INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:67)
[2021-12-08 12:46:07,047] ERROR WorkerSinkTask{id=jdbc-sink-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:601)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: Field 'expenses' doesn't have a default value
java.sql.SQLException: Field 'expenses' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:582)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:330)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: Field 'expenses' doesn't have a default value
java.sql.SQLException: Field 'expenses' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:150)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:102)
	... 11 more
[2021-12-08 12:46:10,072] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2021-12-08 12:46:10,077] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2021-12-08 12:46:10,083] INFO Checking MySql dialect for existence of TABLE "results" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:575)
[2021-12-08 12:46:10,101] INFO Using MySql dialect TABLE "results" present (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:583)
[2021-12-08 12:46:10,136] INFO Checking MySql dialect for type of TABLE "results" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:845)
[2021-12-08 12:46:10,150] INFO Setting metadata for table "results" to Table{name='"results"', type=TABLE columns=[Column{'expenses', isPrimaryKey=false, allowsNull=false, sqlType=DOUBLE}, Column{'profit', isPrimaryKey=false, allowsNull=false, sqlType=DOUBLE}, Column{'revenue', isPrimaryKey=false, allowsNull=false, sqlType=DOUBLE}]} (io.confluent.connect.jdbc.util.TableDefinitions:64)
[2021-12-08 12:46:10,157] WARN Write of 1 records failed, remainingRetries=1 (io.confluent.connect.jdbc.sink.JdbcSinkTask:92)
java.sql.BatchUpdateException: Field 'expenses' doesn't have a default value
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)
	at com.mysql.cj.util.Util.handleNewInstance(Util.java:192)
	at com.mysql.cj.util.Util.getInstance(Util.java:167)
	at com.mysql.cj.util.Util.getInstance(Util.java:174)
	at com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:853)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:435)
	at com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:800)
	at io.confluent.connect.jdbc.sink.BufferedRecords.executeUpdates(BufferedRecords.java:221)
	at io.confluent.connect.jdbc.sink.BufferedRecords.flush(BufferedRecords.java:187)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:80)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:84)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:582)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:330)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLException: Field 'expenses' doesn't have a default value
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:129)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1098)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:832)
	... 17 more
[2021-12-08 12:46:10,169] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2021-12-08 12:46:10,181] INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:67)
[2021-12-08 12:46:10,182] ERROR WorkerSinkTask{id=jdbc-sink-0} RetriableException from SinkTask: (org.apache.kafka.connect.runtime.WorkerSinkTask:601)
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: Field 'expenses' doesn't have a default value
java.sql.SQLException: Field 'expenses' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:108)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:582)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:330)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: Field 'expenses' doesn't have a default value
java.sql.SQLException: Field 'expenses' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:150)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:102)
	... 11 more
[2021-12-08 12:46:13,194] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2021-12-08 12:46:13,199] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2021-12-08 12:46:13,214] INFO Checking MySql dialect for existence of TABLE "results" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:575)
[2021-12-08 12:46:13,231] INFO Using MySql dialect TABLE "results" present (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:583)
[2021-12-08 12:46:13,285] INFO Checking MySql dialect for type of TABLE "results" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:845)
[2021-12-08 12:46:13,304] INFO Setting metadata for table "results" to Table{name='"results"', type=TABLE columns=[Column{'expenses', isPrimaryKey=false, allowsNull=false, sqlType=DOUBLE}, Column{'profit', isPrimaryKey=false, allowsNull=false, sqlType=DOUBLE}, Column{'revenue', isPrimaryKey=false, allowsNull=false, sqlType=DOUBLE}]} (io.confluent.connect.jdbc.util.TableDefinitions:64)
[2021-12-08 12:46:13,306] WARN Write of 1 records failed, remainingRetries=0 (io.confluent.connect.jdbc.sink.JdbcSinkTask:92)
java.sql.BatchUpdateException: Field 'expenses' doesn't have a default value
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)
	at com.mysql.cj.util.Util.handleNewInstance(Util.java:192)
	at com.mysql.cj.util.Util.getInstance(Util.java:167)
	at com.mysql.cj.util.Util.getInstance(Util.java:174)
	at com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:853)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:435)
	at com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:800)
	at io.confluent.connect.jdbc.sink.BufferedRecords.executeUpdates(BufferedRecords.java:221)
	at io.confluent.connect.jdbc.sink.BufferedRecords.flush(BufferedRecords.java:187)
	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:80)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:84)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:582)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:330)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLException: Field 'expenses' doesn't have a default value
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:129)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1098)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:832)
	... 17 more
[2021-12-08 12:46:13,313] ERROR Failing task after exhausting retries; encountered 2 exceptions on last write attempt. For complete details on each exception, please enable DEBUG logging. (io.confluent.connect.jdbc.sink.JdbcSinkTask:113)
[2021-12-08 12:46:13,313] ERROR WorkerSinkTask{id=jdbc-sink-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. Error: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: Field 'expenses' doesn't have a default value
java.sql.SQLException: Field 'expenses' doesn't have a default value
 (org.apache.kafka.connect.runtime.WorkerSinkTask:608)
org.apache.kafka.connect.errors.ConnectException: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: Field 'expenses' doesn't have a default value
java.sql.SQLException: Field 'expenses' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:122)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:582)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:330)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: Field 'expenses' doesn't have a default value
java.sql.SQLException: Field 'expenses' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:150)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:102)
	... 11 more
[2021-12-08 12:46:13,316] ERROR WorkerSinkTask{id=jdbc-sink-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:190)
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:610)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:330)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.apache.kafka.connect.errors.ConnectException: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: Field 'expenses' doesn't have a default value
java.sql.SQLException: Field 'expenses' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:122)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:582)
	... 10 more
Caused by: java.sql.SQLException: Exception chain:
java.sql.BatchUpdateException: Field 'expenses' doesn't have a default value
java.sql.SQLException: Field 'expenses' doesn't have a default value

	at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:150)
	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:102)
	... 11 more
[2021-12-08 12:46:13,316] INFO Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:161)
[2021-12-08 12:46:13,317] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2021-12-08 12:46:13,334] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Revoke previously assigned partitions results-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2021-12-08 12:46:13,335] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Member connector-consumer-jdbc-sink-0-92630dc4-d0aa-4355-9581-5686f06c2446 sending LeaveGroup request to coordinator Nuno-PC:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1045)
[2021-12-08 12:46:13,381] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2021-12-08 12:46:13,381] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2021-12-08 12:46:13,383] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2021-12-08 12:46:13,410] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:46:13,412] INFO App info kafka.consumer for connector-consumer-jdbc-sink-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-12-08 12:46:13,551] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:46:23,422] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:46:23,564] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:46:33,438] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:46:33,578] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:46:43,442] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:46:43,581] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:46:53,449] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:46:53,588] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:47:03,452] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:47:03,594] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:47:13,461] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:47:13,600] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:47:23,465] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:47:23,607] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:47:33,473] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:47:33,610] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:47:43,475] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:47:43,616] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:47:53,482] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:47:53,623] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:48:03,490] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:48:03,628] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:48:13,491] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:48:13,632] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:48:23,500] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:48:23,640] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:48:31,062] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2021-12-08 12:48:31,063] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:327)
[2021-12-08 12:48:31,069] INFO Stopped http_8083@3af9aa66{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:381)
[2021-12-08 12:48:31,074] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:149)
[2021-12-08 12:48:31,075] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:344)
[2021-12-08 12:48:31,075] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:102)
[2021-12-08 12:48:31,076] INFO Stopping task jdbc-source-clients-0 (org.apache.kafka.connect.runtime.Worker:830)
[2021-12-08 12:48:31,078] INFO Stopping connector jdbc-source-clients (org.apache.kafka.connect.runtime.Worker:382)
[2021-12-08 12:48:31,079] INFO Scheduled shutdown for WorkerConnector{id=jdbc-source-clients} (org.apache.kafka.connect.runtime.WorkerConnector:248)
[2021-12-08 12:48:31,079] INFO Stopping table monitoring thread (io.confluent.connect.jdbc.JdbcSourceConnector:176)
[2021-12-08 12:48:31,079] INFO Shutting down thread monitoring tables. (io.confluent.connect.jdbc.source.TableMonitorThread:134)
[2021-12-08 12:48:31,080] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2021-12-08 12:48:31,081] INFO Completed shutdown for WorkerConnector{id=jdbc-source-clients} (org.apache.kafka.connect.runtime.WorkerConnector:268)
[2021-12-08 12:48:31,082] INFO Stopping task jdbc-source-managers-0 (org.apache.kafka.connect.runtime.Worker:830)
[2021-12-08 12:48:31,083] INFO Stopping connector jdbc-source-managers (org.apache.kafka.connect.runtime.Worker:382)
[2021-12-08 12:48:31,083] INFO Scheduled shutdown for WorkerConnector{id=jdbc-source-managers} (org.apache.kafka.connect.runtime.WorkerConnector:248)
[2021-12-08 12:48:31,084] INFO Stopping table monitoring thread (io.confluent.connect.jdbc.JdbcSourceConnector:176)
[2021-12-08 12:48:31,096] INFO Shutting down thread monitoring tables. (io.confluent.connect.jdbc.source.TableMonitorThread:134)
[2021-12-08 12:48:31,098] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2021-12-08 12:48:31,102] INFO Completed shutdown for WorkerConnector{id=jdbc-source-managers} (org.apache.kafka.connect.runtime.WorkerConnector:268)
[2021-12-08 12:48:31,102] INFO Stopping task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:830)
[2021-12-08 12:48:31,104] INFO Stopping connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:382)
[2021-12-08 12:48:31,104] INFO Scheduled shutdown for WorkerConnector{id=jdbc-sink} (org.apache.kafka.connect.runtime.WorkerConnector:248)
[2021-12-08 12:48:31,107] INFO Completed shutdown for WorkerConnector{id=jdbc-sink} (org.apache.kafka.connect.runtime.WorkerConnector:268)
[2021-12-08 12:48:31,111] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:205)
[2021-12-08 12:48:31,112] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:66)
[2021-12-08 12:48:31,112] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2021-12-08 12:48:31,113] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2021-12-08 12:48:31,114] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2021-12-08 12:48:31,115] INFO App info kafka.connect for 169.254.211.170:8083 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-12-08 12:48:31,115] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:226)
[2021-12-08 12:48:31,118] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:120)
[2021-12-08 12:48:31,130] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
[2021-12-08 12:51:10,159] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:68)
[2021-12-08 12:51:10,173] INFO WorkerInfo values: 
	jvm.args = -Xmx256M, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=C:\kafka/logs, -Dlog4j.configuration=file:C:\kafka/config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 17.0.1, 17.0.1+12-LTS-39
	jvm.classpath = C:\kafka\libs\activation-1.1.1.jar;C:\kafka\libs\aopalliance-repackaged-2.6.1.jar;C:\kafka\libs\argparse4j-0.7.0.jar;C:\kafka\libs\audience-annotations-0.5.0.jar;C:\kafka\libs\commons-cli-1.4.jar;C:\kafka\libs\commons-lang3-3.8.1.jar;C:\kafka\libs\connect-api-2.8.1.jar;C:\kafka\libs\connect-basic-auth-extension-2.8.1.jar;C:\kafka\libs\connect-file-2.8.1.jar;C:\kafka\libs\connect-json-2.8.1.jar;C:\kafka\libs\connect-mirror-2.8.1.jar;C:\kafka\libs\connect-mirror-client-2.8.1.jar;C:\kafka\libs\connect-runtime-2.8.1.jar;C:\kafka\libs\connect-transforms-2.8.1.jar;C:\kafka\libs\hk2-api-2.6.1.jar;C:\kafka\libs\hk2-locator-2.6.1.jar;C:\kafka\libs\hk2-utils-2.6.1.jar;C:\kafka\libs\jackson-annotations-2.10.5.jar;C:\kafka\libs\jackson-core-2.10.5.jar;C:\kafka\libs\jackson-databind-2.10.5.1.jar;C:\kafka\libs\jackson-dataformat-csv-2.10.5.jar;C:\kafka\libs\jackson-datatype-jdk8-2.10.5.jar;C:\kafka\libs\jackson-jaxrs-base-2.10.5.jar;C:\kafka\libs\jackson-jaxrs-json-provider-2.10.5.jar;C:\kafka\libs\jackson-module-jaxb-annotations-2.10.5.jar;C:\kafka\libs\jackson-module-paranamer-2.10.5.jar;C:\kafka\libs\jackson-module-scala_2.12-2.10.5.jar;C:\kafka\libs\jakarta.activation-api-1.2.1.jar;C:\kafka\libs\jakarta.annotation-api-1.3.5.jar;C:\kafka\libs\jakarta.inject-2.6.1.jar;C:\kafka\libs\jakarta.validation-api-2.0.2.jar;C:\kafka\libs\jakarta.ws.rs-api-2.1.6.jar;C:\kafka\libs\jakarta.xml.bind-api-2.3.2.jar;C:\kafka\libs\javassist-3.27.0-GA.jar;C:\kafka\libs\javax.servlet-api-3.1.0.jar;C:\kafka\libs\javax.ws.rs-api-2.1.1.jar;C:\kafka\libs\jaxb-api-2.3.0.jar;C:\kafka\libs\jersey-client-2.34.jar;C:\kafka\libs\jersey-common-2.34.jar;C:\kafka\libs\jersey-container-servlet-2.34.jar;C:\kafka\libs\jersey-container-servlet-core-2.34.jar;C:\kafka\libs\jersey-hk2-2.34.jar;C:\kafka\libs\jersey-server-2.34.jar;C:\kafka\libs\jetty-client-9.4.43.v20210629.jar;C:\kafka\libs\jetty-continuation-9.4.43.v20210629.jar;C:\kafka\libs\jetty-http-9.4.43.v20210629.jar;C:\kafka\libs\jetty-io-9.4.43.v20210629.jar;C:\kafka\libs\jetty-security-9.4.43.v20210629.jar;C:\kafka\libs\jetty-server-9.4.43.v20210629.jar;C:\kafka\libs\jetty-servlet-9.4.43.v20210629.jar;C:\kafka\libs\jetty-servlets-9.4.43.v20210629.jar;C:\kafka\libs\jetty-util-9.4.43.v20210629.jar;C:\kafka\libs\jetty-util-ajax-9.4.43.v20210629.jar;C:\kafka\libs\jline-3.12.1.jar;C:\kafka\libs\jopt-simple-5.0.4.jar;C:\kafka\libs\kafka-clients-2.8.1.jar;C:\kafka\libs\kafka-connect-jdbc-10.2.5.jar;C:\kafka\libs\kafka-log4j-appender-2.8.1.jar;C:\kafka\libs\kafka-metadata-2.8.1.jar;C:\kafka\libs\kafka-raft-2.8.1.jar;C:\kafka\libs\kafka-shell-2.8.1.jar;C:\kafka\libs\kafka-streams-2.8.1.jar;C:\kafka\libs\kafka-streams-examples-2.8.1.jar;C:\kafka\libs\kafka-streams-scala_2.12-2.8.1.jar;C:\kafka\libs\kafka-streams-test-utils-2.8.1.jar;C:\kafka\libs\kafka-tools-2.8.1.jar;C:\kafka\libs\kafka_2.12-2.8.1-javadoc.jar;C:\kafka\libs\kafka_2.12-2.8.1-javadoc.jar.asc;C:\kafka\libs\kafka_2.12-2.8.1-sources.jar;C:\kafka\libs\kafka_2.12-2.8.1-sources.jar.asc;C:\kafka\libs\kafka_2.12-2.8.1-test-sources.jar;C:\kafka\libs\kafka_2.12-2.8.1-test-sources.jar.asc;C:\kafka\libs\kafka_2.12-2.8.1-test.jar;C:\kafka\libs\kafka_2.12-2.8.1-test.jar.asc;C:\kafka\libs\kafka_2.12-2.8.1.jar;C:\kafka\libs\kafka_2.12-2.8.1.jar.asc;C:\kafka\libs\log4j-1.2.17.jar;C:\kafka\libs\lz4-java-1.7.1.jar;C:\kafka\libs\maven-artifact-3.8.1.jar;C:\kafka\libs\metrics-core-2.2.0.jar;C:\kafka\libs\mysql-connector-java-8.0.27.jar;C:\kafka\libs\netty-buffer-4.1.62.Final.jar;C:\kafka\libs\netty-codec-4.1.62.Final.jar;C:\kafka\libs\netty-common-4.1.62.Final.jar;C:\kafka\libs\netty-handler-4.1.62.Final.jar;C:\kafka\libs\netty-resolver-4.1.62.Final.jar;C:\kafka\libs\netty-transport-4.1.62.Final.jar;C:\kafka\libs\netty-transport-native-epoll-4.1.62.Final.jar;C:\kafka\libs\netty-transport-native-unix-common-4.1.62.Final.jar;C:\kafka\libs\osgi-resource-locator-1.0.3.jar;C:\kafka\libs\paranamer-2.8.jar;C:\kafka\libs\plexus-utils-3.2.1.jar;C:\kafka\libs\reflections-0.9.12.jar;C:\kafka\libs\rocksdbjni-5.18.4.jar;C:\kafka\libs\scala-collection-compat_2.12-2.3.0.jar;C:\kafka\libs\scala-java8-compat_2.12-0.9.1.jar;C:\kafka\libs\scala-library-2.12.13.jar;C:\kafka\libs\scala-logging_2.12-3.9.2.jar;C:\kafka\libs\scala-reflect-2.12.13.jar;C:\kafka\libs\slf4j-api-1.7.30.jar;C:\kafka\libs\slf4j-log4j12-1.7.30.jar;C:\kafka\libs\snappy-java-1.1.8.1.jar;C:\kafka\libs\zookeeper-3.5.9.jar;C:\kafka\libs\zookeeper-jute-3.5.9.jar;C:\kafka\libs\zstd-jni-1.4.9-1.jar
	os.spec = Windows 10, amd64, 10.0
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2021-12-08 12:51:10,182] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectStandalone:77)
[2021-12-08 12:51:11,696] INFO Registered loader: jdk.internal.loader.ClassLoaders$AppClassLoader@266474c2 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2021-12-08 12:51:11,697] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,700] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,707] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,708] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,709] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,709] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,710] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,710] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,711] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,711] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,712] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,713] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,714] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,714] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,715] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,715] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,716] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,716] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,717] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,718] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,734] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,736] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,751] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,755] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,756] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,769] INFO Added plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,772] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,788] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,789] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,790] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,791] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,797] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,806] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,808] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,809] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,812] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,821] INFO Added plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,823] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,824] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,833] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,833] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,842] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,843] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,844] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,844] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,845] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,846] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,847] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,847] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,848] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,849] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,849] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,850] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:51:11,869] INFO Added aliases 'JdbcSinkConnector' and 'JdbcSink' to plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:51:11,871] INFO Added aliases 'JdbcSourceConnector' and 'JdbcSource' to plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:51:11,875] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:51:11,879] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:51:11,882] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:51:11,883] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:51:11,884] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:51:11,886] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:51:11,899] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:51:11,900] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:51:11,903] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:51:11,905] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:51:11,905] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:51:11,906] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:51:11,912] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:51:11,913] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:51:11,914] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:51:11,914] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:51:11,915] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:51:11,916] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:51:11,917] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:51:11,917] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:51:11,918] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:51:11,930] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:51:11,931] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:51:11,932] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:51:11,933] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:51:11,934] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:51:11,934] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-12-08 12:51:11,935] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:51:11,936] INFO Added aliases 'PredicatedTransformation' and 'Predicated' to plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:51:11,936] INFO Added alias 'Filter' to plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-12-08 12:51:11,937] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-12-08 12:51:11,937] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-12-08 12:51:11,938] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-12-08 12:51:11,939] INFO Added alias 'HasHeaderKey' to plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-12-08 12:51:11,939] INFO Added alias 'RecordIsTombstone' to plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-12-08 12:51:11,940] INFO Added alias 'TopicNameMatches' to plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-12-08 12:51:11,942] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-12-08 12:51:11,947] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:51:11,947] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:51:11,948] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:51:11,977] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	config.providers = []
	connector.client.config.override.policy = None
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	listeners = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = /tmp/connect.offsets
	plugin.path = null
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = null
	rest.port = 8083
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:372)
[2021-12-08 12:51:11,979] WARN Variables cannot be used in the 'plugin.path' property, since the property is used by plugin scanning before the config providers that replace the variables are initialized. The raw value 'null' was used for plugin scanning, as opposed to the transformed value 'null', and this may cause unexpected results. (org.apache.kafka.connect.runtime.WorkerConfig:420)
[2021-12-08 12:51:11,980] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2021-12-08 12:51:11,985] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:372)
[2021-12-08 12:51:12,166] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:51:12,167] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:51:12,170] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:51:12,178] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:51:12,179] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:51:12,179] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:51:12,181] INFO Kafka version: 2.8.1 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-12-08 12:51:12,181] INFO Kafka commitId: 839b886f9b732b15 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-12-08 12:51:12,181] INFO Kafka startTimeMs: 1638967872180 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-12-08 12:51:12,557] INFO Kafka cluster ID: u7MsxwDfR5Ob7FBWXv2wvA (org.apache.kafka.connect.util.ConnectUtils:65)
[2021-12-08 12:51:12,559] INFO App info kafka.admin.client for adminclient-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-12-08 12:51:12,573] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2021-12-08 12:51:12,575] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2021-12-08 12:51:12,590] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2021-12-08 12:51:12,613] INFO Logging initialized @2932ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:170)
[2021-12-08 12:51:12,675] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:132)
[2021-12-08 12:51:12,676] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:204)
[2021-12-08 12:51:12,689] INFO jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 17.0.1+12-LTS-39 (org.eclipse.jetty.server.Server:375)
[2021-12-08 12:51:12,727] INFO Started http_8083@771158fb{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:331)
[2021-12-08 12:51:12,729] INFO Started @3049ms (org.eclipse.jetty.server.Server:415)
[2021-12-08 12:51:12,757] INFO Advertised URI: http://169.254.211.170:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:371)
[2021-12-08 12:51:12,757] INFO REST server listening at http://169.254.211.170:8083/, advertising URL http://169.254.211.170:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2021-12-08 12:51:12,764] INFO Advertised URI: http://169.254.211.170:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:371)
[2021-12-08 12:51:12,770] INFO REST admin endpoints at http://169.254.211.170:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:220)
[2021-12-08 12:51:12,784] INFO Advertised URI: http://169.254.211.170:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:371)
[2021-12-08 12:51:12,785] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy:45)
[2021-12-08 12:51:12,802] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2021-12-08 12:51:12,803] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:372)
[2021-12-08 12:51:12,815] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:51:12,816] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:51:12,816] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:51:12,817] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:51:12,818] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:51:12,821] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:51:12,836] INFO Kafka version: 2.8.1 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-12-08 12:51:12,837] INFO Kafka commitId: 839b886f9b732b15 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-12-08 12:51:12,843] INFO Kafka startTimeMs: 1638967872836 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-12-08 12:51:12,864] INFO Kafka cluster ID: u7MsxwDfR5Ob7FBWXv2wvA (org.apache.kafka.connect.util.ConnectUtils:65)
[2021-12-08 12:51:12,868] INFO App info kafka.admin.client for adminclient-2 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-12-08 12:51:12,879] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2021-12-08 12:51:12,889] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2021-12-08 12:51:12,898] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2021-12-08 12:51:12,910] INFO Kafka version: 2.8.1 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-12-08 12:51:12,910] INFO Kafka commitId: 839b886f9b732b15 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-12-08 12:51:12,911] INFO Kafka startTimeMs: 1638967872910 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-12-08 12:51:13,038] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2021-12-08 12:51:13,041] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2021-12-08 12:51:13,068] INFO Kafka Connect standalone worker initialization took 2907ms (org.apache.kafka.connect.cli.ConnectStandalone:99)
[2021-12-08 12:51:13,069] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2021-12-08 12:51:13,072] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:94)
[2021-12-08 12:51:13,073] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:191)
[2021-12-08 12:51:13,076] INFO Starting FileOffsetBackingStore with file \tmp\connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:58)
[2021-12-08 12:51:13,081] INFO Worker started (org.apache.kafka.connect.runtime.Worker:198)
[2021-12-08 12:51:13,081] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:97)
[2021-12-08 12:51:13,082] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:224)
[2021-12-08 12:51:13,141] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:241)
[2021-12-08 12:51:13,219] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:334)
[2021-12-08 12:51:13,220] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:339)
[2021-12-08 12:51:13,231] INFO node0 Scavenging every 660000ms (org.eclipse.jetty.server.session:132)
[2021-12-08 12:51:13,824] INFO Started o.e.j.s.ServletContextHandler@17b64941{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:915)
[2021-12-08 12:51:13,832] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:319)
[2021-12-08 12:51:13,832] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2021-12-08 12:51:13,866] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:372)
[2021-12-08 12:51:13,884] INFO Creating connector jdbc-source-clients of type io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:271)
[2021-12-08 12:51:13,885] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-clients
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:372)
[2021-12-08 12:51:13,888] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-clients
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:51:13,905] INFO Instantiated connector jdbc-source-clients with version 10.2.5 of type class io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:281)
[2021-12-08 12:51:13,907] INFO Finished creating connector jdbc-source-clients (org.apache.kafka.connect.runtime.Worker:306)
[2021-12-08 12:51:13,911] INFO Starting JDBC Source Connector (io.confluent.connect.jdbc.JdbcSourceConnector:69)
[2021-12-08 12:51:13,912] INFO JdbcSourceConnectorConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:mysql://localhost:3306/project3
	connection.user = projeto
	db.timezone = UTC
	dialect.name = MySqlDatabaseDialect
	incrementing.column.name = 
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 60000
	query = SELECT * FROM clients;
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = []
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = []
	timestamp.delay.interval.ms = 0
	timestamp.initial = null
	topic.prefix = DBinfo
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceConnectorConfig:372)
[2021-12-08 12:51:13,916] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2021-12-08 12:51:14,059] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-clients
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:372)
[2021-12-08 12:51:14,060] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-clients
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:51:14,073] INFO Creating task jdbc-source-clients-0 (org.apache.kafka.connect.runtime.Worker:505)
[2021-12-08 12:51:14,079] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-clients
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:372)
[2021-12-08 12:51:14,082] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-clients
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:51:14,096] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.source.JdbcSourceTask
 (org.apache.kafka.connect.runtime.TaskConfig:372)
[2021-12-08 12:51:14,099] INFO Instantiated task jdbc-source-clients-0 with version 10.2.5 of type io.confluent.connect.jdbc.source.JdbcSourceTask (org.apache.kafka.connect.runtime.Worker:520)
[2021-12-08 12:51:14,100] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2021-12-08 12:51:14,100] INFO Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-source-clients-0 using the worker config (org.apache.kafka.connect.runtime.Worker:533)
[2021-12-08 12:51:14,101] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2021-12-08 12:51:14,102] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-source-clients-0 using the worker config (org.apache.kafka.connect.runtime.Worker:539)
[2021-12-08 12:51:14,102] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-source-clients-0 using the worker config (org.apache.kafka.connect.runtime.Worker:546)
[2021-12-08 12:51:14,111] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-clients
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:372)
[2021-12-08 12:51:14,117] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-clients
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:51:14,125] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:600)
[2021-12-08 12:51:14,135] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-jdbc-source-clients-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:372)
[2021-12-08 12:51:14,177] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-12-08 12:51:14,178] INFO Kafka version: 2.8.1 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-12-08 12:51:14,179] INFO Kafka commitId: 839b886f9b732b15 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-12-08 12:51:14,180] INFO Kafka startTimeMs: 1638967874178 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-12-08 12:51:14,217] INFO [Producer clientId=connector-producer-jdbc-source-clients-0] Cluster ID: u7MsxwDfR5Ob7FBWXv2wvA (org.apache.kafka.clients.Metadata:279)
[2021-12-08 12:51:14,218] INFO Starting JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:88)
[2021-12-08 12:51:14,221] INFO Created connector jdbc-source-clients (org.apache.kafka.connect.cli.ConnectStandalone:109)
[2021-12-08 12:51:14,227] INFO JdbcSourceTaskConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:mysql://localhost:3306/project3
	connection.user = projeto
	db.timezone = UTC
	dialect.name = MySqlDatabaseDialect
	incrementing.column.name = 
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 60000
	query = SELECT * FROM clients;
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = []
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	tables = []
	timestamp.column.name = []
	timestamp.delay.interval.ms = 0
	timestamp.initial = null
	topic.prefix = DBinfo
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceTaskConfig:372)
[2021-12-08 12:51:14,234] INFO Using JDBC dialect MySql (io.confluent.connect.jdbc.source.JdbcSourceTask:105)
[2021-12-08 12:51:14,237] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:372)
[2021-12-08 12:51:14,239] INFO Creating connector jdbc-source-managers of type io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:271)
[2021-12-08 12:51:14,241] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-managers
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:372)
[2021-12-08 12:51:14,244] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-managers
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:51:14,247] INFO Started JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:264)
[2021-12-08 12:51:14,263] INFO WorkerSourceTask{id=jdbc-source-clients-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:232)
[2021-12-08 12:51:14,264] INFO Instantiated connector jdbc-source-managers with version 10.2.5 of type class io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:281)
[2021-12-08 12:51:14,264] INFO WorkerSourceTask{id=jdbc-source-clients-0} Executing source task (org.apache.kafka.connect.runtime.WorkerSourceTask:238)
[2021-12-08 12:51:14,266] INFO Finished creating connector jdbc-source-managers (org.apache.kafka.connect.runtime.Worker:306)
[2021-12-08 12:51:14,266] INFO Starting JDBC Source Connector (io.confluent.connect.jdbc.JdbcSourceConnector:69)
[2021-12-08 12:51:14,267] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2021-12-08 12:51:14,268] INFO JdbcSourceConnectorConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:mysql://localhost:3306/project3
	connection.user = projeto
	db.timezone = UTC
	dialect.name = MySqlDatabaseDialect
	incrementing.column.name = 
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 60000
	query = SELECT * FROM managers;
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = []
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = []
	timestamp.delay.interval.ms = 0
	timestamp.initial = null
	topic.prefix = DBinfo
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceConnectorConfig:372)
[2021-12-08 12:51:14,270] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2021-12-08 12:51:14,283] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-managers
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:372)
[2021-12-08 12:51:14,294] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-managers
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:51:14,295] INFO Creating task jdbc-source-managers-0 (org.apache.kafka.connect.runtime.Worker:505)
[2021-12-08 12:51:14,300] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-managers
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:372)
[2021-12-08 12:51:14,300] INFO Begin using SQL query: SELECT * FROM clients; (io.confluent.connect.jdbc.source.TableQuerier:164)
[2021-12-08 12:51:14,301] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-managers
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:51:14,302] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.source.JdbcSourceTask
 (org.apache.kafka.connect.runtime.TaskConfig:372)
[2021-12-08 12:51:14,303] INFO Instantiated task jdbc-source-managers-0 with version 10.2.5 of type io.confluent.connect.jdbc.source.JdbcSourceTask (org.apache.kafka.connect.runtime.Worker:520)
[2021-12-08 12:51:14,306] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2021-12-08 12:51:14,325] INFO Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-source-managers-0 using the worker config (org.apache.kafka.connect.runtime.Worker:533)
[2021-12-08 12:51:14,329] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2021-12-08 12:51:14,330] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-source-managers-0 using the worker config (org.apache.kafka.connect.runtime.Worker:539)
[2021-12-08 12:51:14,331] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-source-managers-0 using the worker config (org.apache.kafka.connect.runtime.Worker:546)
[2021-12-08 12:51:14,333] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-managers
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:372)
[2021-12-08 12:51:14,334] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-managers
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:51:14,335] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:600)
[2021-12-08 12:51:14,350] ERROR Non-transient SQL exception while running query for table: BulkTableQuerier{table='null', query='SELECT * FROM clients;', topicPrefix='DBinfo'} (io.confluent.connect.jdbc.source.JdbcSourceTask:412)
java.sql.SQLSyntaxErrorException: Table 'project3.clients' doesn't exist
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeQuery(ClientPreparedStatement.java:1009)
	at io.confluent.connect.jdbc.source.BulkTableQuerier.executeQuery(BulkTableQuerier.java:79)
	at io.confluent.connect.jdbc.source.TableQuerier.maybeStartQuery(TableQuerier.java:98)
	at io.confluent.connect.jdbc.source.BulkTableQuerier.maybeStartQuery(BulkTableQuerier.java:39)
	at io.confluent.connect.jdbc.source.JdbcSourceTask.poll(JdbcSourceTask.java:379)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.poll(WorkerSourceTask.java:296)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.execute(WorkerSourceTask.java:253)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
[2021-12-08 12:51:14,350] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-jdbc-source-managers-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:372)
[2021-12-08 12:51:14,356] INFO Closing resources for JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:332)
[2021-12-08 12:51:14,360] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2021-12-08 12:51:14,366] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:51:14,369] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-12-08 12:51:14,375] ERROR WorkerSourceTask{id=jdbc-source-clients-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:190)
org.apache.kafka.connect.errors.ConnectException: java.sql.SQLSyntaxErrorException: Table 'project3.clients' doesn't exist
	at io.confluent.connect.jdbc.source.JdbcSourceTask.poll(JdbcSourceTask.java:417)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.poll(WorkerSourceTask.java:296)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.execute(WorkerSourceTask.java:253)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLSyntaxErrorException: Table 'project3.clients' doesn't exist
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeQuery(ClientPreparedStatement.java:1009)
	at io.confluent.connect.jdbc.source.BulkTableQuerier.executeQuery(BulkTableQuerier.java:79)
	at io.confluent.connect.jdbc.source.TableQuerier.maybeStartQuery(TableQuerier.java:98)
	at io.confluent.connect.jdbc.source.BulkTableQuerier.maybeStartQuery(BulkTableQuerier.java:39)
	at io.confluent.connect.jdbc.source.JdbcSourceTask.poll(JdbcSourceTask.java:379)
	... 9 more
[2021-12-08 12:51:14,376] INFO Kafka version: 2.8.1 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-12-08 12:51:14,380] INFO Stopping JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:320)
[2021-12-08 12:51:14,380] INFO Kafka commitId: 839b886f9b732b15 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-12-08 12:51:14,381] INFO Closing resources for JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:332)
[2021-12-08 12:51:14,382] INFO [Producer clientId=connector-producer-jdbc-source-managers-0] Cluster ID: u7MsxwDfR5Ob7FBWXv2wvA (org.apache.kafka.clients.Metadata:279)
[2021-12-08 12:51:14,382] INFO Kafka startTimeMs: 1638967874376 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-12-08 12:51:14,383] INFO [Producer clientId=connector-producer-jdbc-source-clients-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1204)
[2021-12-08 12:51:14,388] INFO Created connector jdbc-source-managers (org.apache.kafka.connect.cli.ConnectStandalone:109)
[2021-12-08 12:51:14,388] INFO Starting JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:88)
[2021-12-08 12:51:14,391] INFO JdbcSourceTaskConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:mysql://localhost:3306/project3
	connection.user = projeto
	db.timezone = UTC
	dialect.name = MySqlDatabaseDialect
	incrementing.column.name = 
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 60000
	query = SELECT * FROM managers;
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = []
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	tables = []
	timestamp.column.name = []
	timestamp.delay.interval.ms = 0
	timestamp.initial = null
	topic.prefix = DBinfo
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceTaskConfig:372)
[2021-12-08 12:51:14,391] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2021-12-08 12:51:14,394] INFO Using JDBC dialect MySql (io.confluent.connect.jdbc.source.JdbcSourceTask:105)
[2021-12-08 12:51:14,404] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:372)
[2021-12-08 12:51:14,398] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2021-12-08 12:51:14,412] INFO Creating connector jdbc-sink of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:271)
[2021-12-08 12:51:14,409] INFO Started JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:264)
[2021-12-08 12:51:14,413] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	topics = [results]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2021-12-08 12:51:14,412] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2021-12-08 12:51:14,415] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	topics = [results]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:51:14,414] INFO WorkerSourceTask{id=jdbc-source-managers-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:232)
[2021-12-08 12:51:14,416] INFO Instantiated connector jdbc-sink with version 10.2.5 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:281)
[2021-12-08 12:51:14,416] INFO App info kafka.producer for connector-producer-jdbc-source-clients-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-12-08 12:51:14,418] INFO Finished creating connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:306)
[2021-12-08 12:51:14,417] INFO WorkerSourceTask{id=jdbc-source-managers-0} Executing source task (org.apache.kafka.connect.runtime.WorkerSourceTask:238)
[2021-12-08 12:51:14,422] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2021-12-08 12:51:14,422] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	topics = [results]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2021-12-08 12:51:14,431] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	topics = [results]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:51:14,436] INFO Begin using SQL query: SELECT * FROM managers; (io.confluent.connect.jdbc.source.TableQuerier:164)
[2021-12-08 12:51:14,442] INFO Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:44)
[2021-12-08 12:51:14,445] INFO Creating task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:505)
[2021-12-08 12:51:14,446] ERROR Non-transient SQL exception while running query for table: BulkTableQuerier{table='null', query='SELECT * FROM managers;', topicPrefix='DBinfo'} (io.confluent.connect.jdbc.source.JdbcSourceTask:412)
java.sql.SQLSyntaxErrorException: Table 'project3.managers' doesn't exist
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeQuery(ClientPreparedStatement.java:1009)
	at io.confluent.connect.jdbc.source.BulkTableQuerier.executeQuery(BulkTableQuerier.java:79)
	at io.confluent.connect.jdbc.source.TableQuerier.maybeStartQuery(TableQuerier.java:98)
	at io.confluent.connect.jdbc.source.BulkTableQuerier.maybeStartQuery(BulkTableQuerier.java:39)
	at io.confluent.connect.jdbc.source.JdbcSourceTask.poll(JdbcSourceTask.java:379)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.poll(WorkerSourceTask.java:296)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.execute(WorkerSourceTask.java:253)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
[2021-12-08 12:51:14,447] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:372)
[2021-12-08 12:51:14,447] INFO Closing resources for JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:332)
[2021-12-08 12:51:14,448] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:51:14,449] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2021-12-08 12:51:14,450] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:372)
[2021-12-08 12:51:14,451] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:51:14,451] INFO Instantiated task jdbc-sink-0 with version 10.2.5 of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:520)
[2021-12-08 12:51:14,453] ERROR WorkerSourceTask{id=jdbc-source-managers-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:190)
org.apache.kafka.connect.errors.ConnectException: java.sql.SQLSyntaxErrorException: Table 'project3.managers' doesn't exist
	at io.confluent.connect.jdbc.source.JdbcSourceTask.poll(JdbcSourceTask.java:417)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.poll(WorkerSourceTask.java:296)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.execute(WorkerSourceTask.java:253)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLSyntaxErrorException: Table 'project3.managers' doesn't exist
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeQuery(ClientPreparedStatement.java:1009)
	at io.confluent.connect.jdbc.source.BulkTableQuerier.executeQuery(BulkTableQuerier.java:79)
	at io.confluent.connect.jdbc.source.TableQuerier.maybeStartQuery(TableQuerier.java:98)
	at io.confluent.connect.jdbc.source.BulkTableQuerier.maybeStartQuery(BulkTableQuerier.java:39)
	at io.confluent.connect.jdbc.source.JdbcSourceTask.poll(JdbcSourceTask.java:379)
	... 9 more
[2021-12-08 12:51:14,456] INFO Stopping JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:320)
[2021-12-08 12:51:14,457] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:372)
[2021-12-08 12:51:14,461] INFO Closing resources for JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:332)
[2021-12-08 12:51:14,470] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2021-12-08 12:51:14,475] INFO [Producer clientId=connector-producer-jdbc-source-managers-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1204)
[2021-12-08 12:51:14,475] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-sink-0 using the connector config (org.apache.kafka.connect.runtime.Worker:535)
[2021-12-08 12:51:14,476] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-sink-0 using the connector config (org.apache.kafka.connect.runtime.Worker:541)
[2021-12-08 12:51:14,477] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:546)
[2021-12-08 12:51:14,478] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2021-12-08 12:51:14,479] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2021-12-08 12:51:14,479] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:626)
[2021-12-08 12:51:14,479] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2021-12-08 12:51:14,480] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	topics = [results]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2021-12-08 12:51:14,481] INFO App info kafka.producer for connector-producer-jdbc-source-managers-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-12-08 12:51:14,482] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	topics = [results]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:51:14,497] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-jdbc-sink-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-jdbc-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:372)
[2021-12-08 12:51:14,548] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-12-08 12:51:14,548] INFO Kafka version: 2.8.1 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-12-08 12:51:14,548] INFO Kafka commitId: 839b886f9b732b15 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-12-08 12:51:14,549] INFO Kafka startTimeMs: 1638967874548 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-12-08 12:51:14,559] INFO Created connector jdbc-sink (org.apache.kafka.connect.cli.ConnectStandalone:109)
[2021-12-08 12:51:14,561] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Subscribed to topic(s): results (org.apache.kafka.clients.consumer.KafkaConsumer:965)
[2021-12-08 12:51:14,567] INFO Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:48)
[2021-12-08 12:51:14,568] INFO JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = false
	batch.size = 3000
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:mysql://localhost:3306/project3
	connection.user = projeto
	db.timezone = UTC
	delete.enabled = false
	dialect.name = MySqlDatabaseDialect
	fields.whitelist = [profit, revenue]
	insert.mode = insert
	max.retries = 10
	pk.fields = []
	pk.mode = none
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = ${topic}
	table.types = [TABLE]
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:372)
[2021-12-08 12:51:14,591] INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:67)
[2021-12-08 12:51:14,599] INFO WorkerSinkTask{id=jdbc-sink-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:310)
[2021-12-08 12:51:14,599] INFO WorkerSinkTask{id=jdbc-sink-0} Executing sink task (org.apache.kafka.connect.runtime.WorkerSinkTask:196)
[2021-12-08 12:51:14,647] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Cluster ID: u7MsxwDfR5Ob7FBWXv2wvA (org.apache.kafka.clients.Metadata:279)
[2021-12-08 12:51:14,651] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Discovered group coordinator Nuno-PC:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:850)
[2021-12-08 12:51:14,661] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-12-08 12:51:14,746] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-12-08 12:51:14,785] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Successfully joined group with generation Generation{generationId=7, memberId='connector-consumer-jdbc-sink-0-6eccc57c-4ce4-473a-b5d9-0d41049f0e44', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-12-08 12:51:14,792] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Finished assignment for group at generation 7: {connector-consumer-jdbc-sink-0-6eccc57c-4ce4-473a-b5d9-0d41049f0e44=Assignment(partitions=[results-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:626)
[2021-12-08 12:51:14,889] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Successfully synced group in generation Generation{generationId=7, memberId='connector-consumer-jdbc-sink-0-6eccc57c-4ce4-473a-b5d9-0d41049f0e44', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:760)
[2021-12-08 12:51:14,890] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Notifying assignor about the new Assignment(partitions=[results-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-12-08 12:51:14,894] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Adding newly assigned partitions: results-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-12-08 12:51:14,929] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Setting offset for partition results-0 to the committed offset FetchPosition{offset=1, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[Nuno-PC:9092 (id: 0 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:820)
[2021-12-08 12:51:15,038] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2021-12-08 12:51:15,044] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2021-12-08 12:51:15,066] INFO Checking MySql dialect for existence of TABLE "results" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:575)
[2021-12-08 12:51:15,084] INFO Using MySql dialect TABLE "results" absent (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:583)
[2021-12-08 12:51:15,093] INFO Creating table with sql: CREATE TABLE `results` (
`revenue` DOUBLE NOT NULL,
`profit` DOUBLE NOT NULL) (io.confluent.connect.jdbc.sink.DbStructure:122)
[2021-12-08 12:51:15,158] INFO Checking MySql dialect for existence of TABLE "results" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:575)
[2021-12-08 12:51:15,172] INFO Using MySql dialect TABLE "results" present (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:583)
[2021-12-08 12:51:15,216] INFO Checking MySql dialect for type of TABLE "results" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:845)
[2021-12-08 12:51:15,233] INFO Setting metadata for table "results" to Table{name='"results"', type=TABLE columns=[Column{'profit', isPrimaryKey=false, allowsNull=false, sqlType=DOUBLE}, Column{'revenue', isPrimaryKey=false, allowsNull=false, sqlType=DOUBLE}]} (io.confluent.connect.jdbc.util.TableDefinitions:64)
[2021-12-08 12:51:24,225] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:51:24,395] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:51:34,237] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:51:34,407] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:51:44,247] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:51:44,416] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:51:54,251] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:51:54,422] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:52:04,264] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:52:04,435] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:52:14,276] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:52:14,448] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:52:24,292] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:52:24,463] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:52:34,293] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:52:34,478] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:52:44,307] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:52:44,492] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:52:54,320] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:52:54,506] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:52:57,673] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Error sending fetch request (sessionId=1187942981, epoch=201) to node 0: (org.apache.kafka.clients.FetchSessionHandler:481)
org.apache.kafka.common.errors.DisconnectException
[2021-12-08 12:52:57,677] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Group coordinator Nuno-PC:9092 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable.isDisconnected: true. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:913)
[2021-12-08 12:52:59,811] WARN [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Connection to node 0 (Nuno-PC/169.254.211.170:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:776)
[2021-12-08 12:53:01,955] WARN [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Connection to node 0 (Nuno-PC/192.168.56.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:776)
[2021-12-08 12:53:04,232] WARN [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Connection to node 0 (Nuno-PC/192.168.1.73:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:776)
[2021-12-08 12:53:04,325] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:53:04,509] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:53:06,628] WARN [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Connection to node 0 (Nuno-PC/169.254.211.170:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:776)
[2021-12-08 12:53:07,286] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Error sending fetch request (sessionId=1187942981, epoch=INITIAL) to node 0: (org.apache.kafka.clients.FetchSessionHandler:481)
org.apache.kafka.common.errors.DisconnectException
[2021-12-08 12:53:09,335] WARN [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Connection to node 0 (Nuno-PC/192.168.56.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:776)
[2021-12-08 12:53:09,582] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2021-12-08 12:53:09,583] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:327)
[2021-12-08 12:53:09,589] INFO Stopped http_8083@771158fb{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:381)
[2021-12-08 12:53:09,595] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:149)
[2021-12-08 12:53:09,598] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:344)
[2021-12-08 12:53:09,605] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:102)
[2021-12-08 12:53:09,605] INFO Stopping task jdbc-source-clients-0 (org.apache.kafka.connect.runtime.Worker:830)
[2021-12-08 12:53:09,608] INFO Stopping connector jdbc-source-clients (org.apache.kafka.connect.runtime.Worker:382)
[2021-12-08 12:53:09,608] INFO Scheduled shutdown for WorkerConnector{id=jdbc-source-clients} (org.apache.kafka.connect.runtime.WorkerConnector:248)
[2021-12-08 12:53:09,609] INFO Stopping table monitoring thread (io.confluent.connect.jdbc.JdbcSourceConnector:176)
[2021-12-08 12:53:09,617] INFO Shutting down thread monitoring tables. (io.confluent.connect.jdbc.source.TableMonitorThread:134)
[2021-12-08 12:53:09,626] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2021-12-08 12:53:09,633] INFO Completed shutdown for WorkerConnector{id=jdbc-source-clients} (org.apache.kafka.connect.runtime.WorkerConnector:268)
[2021-12-08 12:53:09,636] INFO Stopping task jdbc-source-managers-0 (org.apache.kafka.connect.runtime.Worker:830)
[2021-12-08 12:53:09,644] INFO Stopping connector jdbc-source-managers (org.apache.kafka.connect.runtime.Worker:382)
[2021-12-08 12:53:09,649] INFO Scheduled shutdown for WorkerConnector{id=jdbc-source-managers} (org.apache.kafka.connect.runtime.WorkerConnector:248)
[2021-12-08 12:53:09,650] INFO Stopping table monitoring thread (io.confluent.connect.jdbc.JdbcSourceConnector:176)
[2021-12-08 12:53:09,659] INFO Shutting down thread monitoring tables. (io.confluent.connect.jdbc.source.TableMonitorThread:134)
[2021-12-08 12:53:09,665] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2021-12-08 12:53:09,676] INFO Completed shutdown for WorkerConnector{id=jdbc-source-managers} (org.apache.kafka.connect.runtime.WorkerConnector:268)
[2021-12-08 12:53:09,682] INFO Stopping task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:830)
[2021-12-08 12:53:09,693] INFO Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask:161)
[2021-12-08 12:53:09,697] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2021-12-08 12:53:09,709] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Revoke previously assigned partitions results-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2021-12-08 12:53:09,711] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2021-12-08 12:53:09,712] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2021-12-08 12:53:09,716] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2021-12-08 12:53:09,718] INFO App info kafka.consumer for connector-consumer-jdbc-sink-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-12-08 12:53:09,721] INFO Stopping connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:382)
[2021-12-08 12:53:09,726] INFO Scheduled shutdown for WorkerConnector{id=jdbc-sink} (org.apache.kafka.connect.runtime.WorkerConnector:248)
[2021-12-08 12:53:09,735] INFO Completed shutdown for WorkerConnector{id=jdbc-sink} (org.apache.kafka.connect.runtime.WorkerConnector:268)
[2021-12-08 12:53:09,740] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:205)
[2021-12-08 12:53:09,756] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:66)
[2021-12-08 12:53:09,760] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2021-12-08 12:53:09,773] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2021-12-08 12:53:09,774] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2021-12-08 12:53:09,775] INFO App info kafka.connect for 169.254.211.170:8083 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-12-08 12:53:09,775] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:226)
[2021-12-08 12:53:09,782] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:120)
[2021-12-08 12:53:09,788] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
[2021-12-08 12:53:51,462] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:68)
[2021-12-08 12:53:51,477] INFO WorkerInfo values: 
	jvm.args = -Xmx256M, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=C:\kafka/logs, -Dlog4j.configuration=file:C:\kafka/config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 17.0.1, 17.0.1+12-LTS-39
	jvm.classpath = C:\kafka\libs\activation-1.1.1.jar;C:\kafka\libs\aopalliance-repackaged-2.6.1.jar;C:\kafka\libs\argparse4j-0.7.0.jar;C:\kafka\libs\audience-annotations-0.5.0.jar;C:\kafka\libs\commons-cli-1.4.jar;C:\kafka\libs\commons-lang3-3.8.1.jar;C:\kafka\libs\connect-api-2.8.1.jar;C:\kafka\libs\connect-basic-auth-extension-2.8.1.jar;C:\kafka\libs\connect-file-2.8.1.jar;C:\kafka\libs\connect-json-2.8.1.jar;C:\kafka\libs\connect-mirror-2.8.1.jar;C:\kafka\libs\connect-mirror-client-2.8.1.jar;C:\kafka\libs\connect-runtime-2.8.1.jar;C:\kafka\libs\connect-transforms-2.8.1.jar;C:\kafka\libs\hk2-api-2.6.1.jar;C:\kafka\libs\hk2-locator-2.6.1.jar;C:\kafka\libs\hk2-utils-2.6.1.jar;C:\kafka\libs\jackson-annotations-2.10.5.jar;C:\kafka\libs\jackson-core-2.10.5.jar;C:\kafka\libs\jackson-databind-2.10.5.1.jar;C:\kafka\libs\jackson-dataformat-csv-2.10.5.jar;C:\kafka\libs\jackson-datatype-jdk8-2.10.5.jar;C:\kafka\libs\jackson-jaxrs-base-2.10.5.jar;C:\kafka\libs\jackson-jaxrs-json-provider-2.10.5.jar;C:\kafka\libs\jackson-module-jaxb-annotations-2.10.5.jar;C:\kafka\libs\jackson-module-paranamer-2.10.5.jar;C:\kafka\libs\jackson-module-scala_2.12-2.10.5.jar;C:\kafka\libs\jakarta.activation-api-1.2.1.jar;C:\kafka\libs\jakarta.annotation-api-1.3.5.jar;C:\kafka\libs\jakarta.inject-2.6.1.jar;C:\kafka\libs\jakarta.validation-api-2.0.2.jar;C:\kafka\libs\jakarta.ws.rs-api-2.1.6.jar;C:\kafka\libs\jakarta.xml.bind-api-2.3.2.jar;C:\kafka\libs\javassist-3.27.0-GA.jar;C:\kafka\libs\javax.servlet-api-3.1.0.jar;C:\kafka\libs\javax.ws.rs-api-2.1.1.jar;C:\kafka\libs\jaxb-api-2.3.0.jar;C:\kafka\libs\jersey-client-2.34.jar;C:\kafka\libs\jersey-common-2.34.jar;C:\kafka\libs\jersey-container-servlet-2.34.jar;C:\kafka\libs\jersey-container-servlet-core-2.34.jar;C:\kafka\libs\jersey-hk2-2.34.jar;C:\kafka\libs\jersey-server-2.34.jar;C:\kafka\libs\jetty-client-9.4.43.v20210629.jar;C:\kafka\libs\jetty-continuation-9.4.43.v20210629.jar;C:\kafka\libs\jetty-http-9.4.43.v20210629.jar;C:\kafka\libs\jetty-io-9.4.43.v20210629.jar;C:\kafka\libs\jetty-security-9.4.43.v20210629.jar;C:\kafka\libs\jetty-server-9.4.43.v20210629.jar;C:\kafka\libs\jetty-servlet-9.4.43.v20210629.jar;C:\kafka\libs\jetty-servlets-9.4.43.v20210629.jar;C:\kafka\libs\jetty-util-9.4.43.v20210629.jar;C:\kafka\libs\jetty-util-ajax-9.4.43.v20210629.jar;C:\kafka\libs\jline-3.12.1.jar;C:\kafka\libs\jopt-simple-5.0.4.jar;C:\kafka\libs\kafka-clients-2.8.1.jar;C:\kafka\libs\kafka-connect-jdbc-10.2.5.jar;C:\kafka\libs\kafka-log4j-appender-2.8.1.jar;C:\kafka\libs\kafka-metadata-2.8.1.jar;C:\kafka\libs\kafka-raft-2.8.1.jar;C:\kafka\libs\kafka-shell-2.8.1.jar;C:\kafka\libs\kafka-streams-2.8.1.jar;C:\kafka\libs\kafka-streams-examples-2.8.1.jar;C:\kafka\libs\kafka-streams-scala_2.12-2.8.1.jar;C:\kafka\libs\kafka-streams-test-utils-2.8.1.jar;C:\kafka\libs\kafka-tools-2.8.1.jar;C:\kafka\libs\kafka_2.12-2.8.1-javadoc.jar;C:\kafka\libs\kafka_2.12-2.8.1-javadoc.jar.asc;C:\kafka\libs\kafka_2.12-2.8.1-sources.jar;C:\kafka\libs\kafka_2.12-2.8.1-sources.jar.asc;C:\kafka\libs\kafka_2.12-2.8.1-test-sources.jar;C:\kafka\libs\kafka_2.12-2.8.1-test-sources.jar.asc;C:\kafka\libs\kafka_2.12-2.8.1-test.jar;C:\kafka\libs\kafka_2.12-2.8.1-test.jar.asc;C:\kafka\libs\kafka_2.12-2.8.1.jar;C:\kafka\libs\kafka_2.12-2.8.1.jar.asc;C:\kafka\libs\log4j-1.2.17.jar;C:\kafka\libs\lz4-java-1.7.1.jar;C:\kafka\libs\maven-artifact-3.8.1.jar;C:\kafka\libs\metrics-core-2.2.0.jar;C:\kafka\libs\mysql-connector-java-8.0.27.jar;C:\kafka\libs\netty-buffer-4.1.62.Final.jar;C:\kafka\libs\netty-codec-4.1.62.Final.jar;C:\kafka\libs\netty-common-4.1.62.Final.jar;C:\kafka\libs\netty-handler-4.1.62.Final.jar;C:\kafka\libs\netty-resolver-4.1.62.Final.jar;C:\kafka\libs\netty-transport-4.1.62.Final.jar;C:\kafka\libs\netty-transport-native-epoll-4.1.62.Final.jar;C:\kafka\libs\netty-transport-native-unix-common-4.1.62.Final.jar;C:\kafka\libs\osgi-resource-locator-1.0.3.jar;C:\kafka\libs\paranamer-2.8.jar;C:\kafka\libs\plexus-utils-3.2.1.jar;C:\kafka\libs\reflections-0.9.12.jar;C:\kafka\libs\rocksdbjni-5.18.4.jar;C:\kafka\libs\scala-collection-compat_2.12-2.3.0.jar;C:\kafka\libs\scala-java8-compat_2.12-0.9.1.jar;C:\kafka\libs\scala-library-2.12.13.jar;C:\kafka\libs\scala-logging_2.12-3.9.2.jar;C:\kafka\libs\scala-reflect-2.12.13.jar;C:\kafka\libs\slf4j-api-1.7.30.jar;C:\kafka\libs\slf4j-log4j12-1.7.30.jar;C:\kafka\libs\snappy-java-1.1.8.1.jar;C:\kafka\libs\zookeeper-3.5.9.jar;C:\kafka\libs\zookeeper-jute-3.5.9.jar;C:\kafka\libs\zstd-jni-1.4.9-1.jar
	os.spec = Windows 10, amd64, 10.0
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2021-12-08 12:53:51,488] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectStandalone:77)
[2021-12-08 12:53:52,960] INFO Registered loader: jdk.internal.loader.ClassLoaders$AppClassLoader@266474c2 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2021-12-08 12:53:52,960] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:52,963] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:52,971] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:52,971] INFO Added plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:52,972] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:52,972] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:52,974] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:52,974] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:52,980] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:52,981] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:52,992] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:52,994] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:53,002] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:53,004] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:53,013] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:53,016] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:53,016] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:53,026] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:53,027] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:53,033] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:53,036] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:53,036] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:53,049] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:53,055] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:53,068] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:53,071] INFO Added plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:53,083] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:53,084] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:53,085] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:53,088] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:53,089] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:53,090] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:53,093] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:53,100] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:53,108] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:53,109] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:53,110] INFO Added plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:53,111] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:53,112] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:53,112] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:53,113] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:53,113] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:53,114] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:53,115] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:53,115] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:53,116] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:53,117] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:53,118] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:53,120] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:53,121] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:53,122] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:53,123] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:53,123] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2021-12-08 12:53:53,145] INFO Added aliases 'JdbcSinkConnector' and 'JdbcSink' to plugin 'io.confluent.connect.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:53:53,147] INFO Added aliases 'JdbcSourceConnector' and 'JdbcSource' to plugin 'io.confluent.connect.jdbc.JdbcSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:53:53,148] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:53:53,148] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:53:53,149] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:53:53,150] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:53:53,151] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:53:53,151] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:53:53,152] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:53:53,155] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:53:53,156] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:53:53,163] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:53:53,169] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:53:53,169] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:53:53,170] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:53:53,171] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:53:53,171] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:53:53,173] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:53:53,183] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:53:53,191] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:53:53,192] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:53:53,193] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:53:53,198] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:53:53,199] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:53:53,200] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:53:53,200] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:53:53,201] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:53:53,202] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:53:53,202] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-12-08 12:53:53,203] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:53:53,214] INFO Added aliases 'PredicatedTransformation' and 'Predicated' to plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:53:53,215] INFO Added alias 'Filter' to plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-12-08 12:53:53,216] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-12-08 12:53:53,217] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-12-08 12:53:53,217] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-12-08 12:53:53,217] INFO Added alias 'HasHeaderKey' to plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-12-08 12:53:53,220] INFO Added alias 'RecordIsTombstone' to plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-12-08 12:53:53,222] INFO Added alias 'TopicNameMatches' to plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-12-08 12:53:53,223] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:424)
[2021-12-08 12:53:53,223] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:53:53,225] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:53:53,230] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2021-12-08 12:53:53,269] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	config.providers = []
	connector.client.config.override.policy = None
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	listeners = null
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = /tmp/connect.offsets
	plugin.path = null
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = null
	rest.port = 8083
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:372)
[2021-12-08 12:53:53,272] WARN Variables cannot be used in the 'plugin.path' property, since the property is used by plugin scanning before the config providers that replace the variables are initialized. The raw value 'null' was used for plugin scanning, as opposed to the transformed value 'null', and this may cause unexpected results. (org.apache.kafka.connect.runtime.WorkerConfig:420)
[2021-12-08 12:53:53,281] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2021-12-08 12:53:53,290] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:372)
[2021-12-08 12:53:53,445] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:53:53,445] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:53:53,448] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:53:53,456] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:53:53,457] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:53:53,458] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:53:53,464] INFO Kafka version: 2.8.1 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-12-08 12:53:53,465] INFO Kafka commitId: 839b886f9b732b15 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-12-08 12:53:53,465] INFO Kafka startTimeMs: 1638968033464 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-12-08 12:54:01,593] INFO Kafka cluster ID: u7MsxwDfR5Ob7FBWXv2wvA (org.apache.kafka.connect.util.ConnectUtils:65)
[2021-12-08 12:54:01,609] INFO App info kafka.admin.client for adminclient-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-12-08 12:54:01,778] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2021-12-08 12:54:01,794] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2021-12-08 12:54:01,866] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2021-12-08 12:54:02,024] INFO Logging initialized @11190ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:170)
[2021-12-08 12:54:02,292] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:132)
[2021-12-08 12:54:02,297] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:204)
[2021-12-08 12:54:02,368] INFO jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 17.0.1+12-LTS-39 (org.eclipse.jetty.server.Server:375)
[2021-12-08 12:54:02,524] INFO Started http_8083@771158fb{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:331)
[2021-12-08 12:54:02,531] INFO Started @11694ms (org.eclipse.jetty.server.Server:415)
[2021-12-08 12:54:02,751] INFO Advertised URI: http://169.254.211.170:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:371)
[2021-12-08 12:54:02,753] INFO REST server listening at http://169.254.211.170:8083/, advertising URL http://169.254.211.170:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2021-12-08 12:54:02,767] INFO Advertised URI: http://169.254.211.170:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:371)
[2021-12-08 12:54:02,817] INFO REST admin endpoints at http://169.254.211.170:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:220)
[2021-12-08 12:54:02,828] INFO Advertised URI: http://169.254.211.170:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:371)
[2021-12-08 12:54:02,841] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy:45)
[2021-12-08 12:54:02,942] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2021-12-08 12:54:02,945] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:372)
[2021-12-08 12:54:03,009] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:54:03,030] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:54:03,066] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:54:03,082] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:54:03,116] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:54:03,176] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2021-12-08 12:54:03,201] INFO Kafka version: 2.8.1 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-12-08 12:54:03,230] INFO Kafka commitId: 839b886f9b732b15 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-12-08 12:54:03,235] INFO Kafka startTimeMs: 1638968043201 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-12-08 12:54:03,405] INFO Kafka cluster ID: u7MsxwDfR5Ob7FBWXv2wvA (org.apache.kafka.connect.util.ConnectUtils:65)
[2021-12-08 12:54:03,414] INFO App info kafka.admin.client for adminclient-2 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-12-08 12:54:03,455] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2021-12-08 12:54:03,481] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2021-12-08 12:54:03,527] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2021-12-08 12:54:03,564] INFO Kafka version: 2.8.1 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-12-08 12:54:03,585] INFO Kafka commitId: 839b886f9b732b15 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-12-08 12:54:03,626] INFO Kafka startTimeMs: 1638968043563 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-12-08 12:54:04,312] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2021-12-08 12:54:04,338] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2021-12-08 12:54:04,401] INFO Kafka Connect standalone worker initialization took 12936ms (org.apache.kafka.connect.cli.ConnectStandalone:99)
[2021-12-08 12:54:04,403] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2021-12-08 12:54:04,434] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:94)
[2021-12-08 12:54:04,436] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:191)
[2021-12-08 12:54:04,461] INFO Starting FileOffsetBackingStore with file \tmp\connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:58)
[2021-12-08 12:54:04,562] INFO Worker started (org.apache.kafka.connect.runtime.Worker:198)
[2021-12-08 12:54:04,572] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:97)
[2021-12-08 12:54:04,608] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:224)
[2021-12-08 12:54:04,899] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:241)
[2021-12-08 12:54:05,403] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:334)
[2021-12-08 12:54:05,407] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:339)
[2021-12-08 12:54:05,466] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session:132)
[2021-12-08 12:54:07,821] INFO Started o.e.j.s.ServletContextHandler@17b64941{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:915)
[2021-12-08 12:54:07,822] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:319)
[2021-12-08 12:54:07,823] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2021-12-08 12:54:07,966] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:372)
[2021-12-08 12:54:08,019] INFO Creating connector jdbc-source-clients of type io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:271)
[2021-12-08 12:54:08,024] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-clients
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:372)
[2021-12-08 12:54:08,030] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-clients
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:54:08,046] INFO Instantiated connector jdbc-source-clients with version 10.2.5 of type class io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:281)
[2021-12-08 12:54:08,047] INFO Finished creating connector jdbc-source-clients (org.apache.kafka.connect.runtime.Worker:306)
[2021-12-08 12:54:08,093] INFO Starting JDBC Source Connector (io.confluent.connect.jdbc.JdbcSourceConnector:69)
[2021-12-08 12:54:08,095] INFO JdbcSourceConnectorConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:mysql://localhost:3306/project3
	connection.user = projeto
	db.timezone = UTC
	dialect.name = MySqlDatabaseDialect
	incrementing.column.name = 
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 60000
	query = SELECT * FROM clients;
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = []
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = []
	timestamp.delay.interval.ms = 0
	timestamp.initial = null
	topic.prefix = DBinfo
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceConnectorConfig:372)
[2021-12-08 12:54:08,125] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2021-12-08 12:54:08,740] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-clients
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:372)
[2021-12-08 12:54:08,741] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-clients
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:54:08,754] INFO Creating task jdbc-source-clients-0 (org.apache.kafka.connect.runtime.Worker:505)
[2021-12-08 12:54:08,757] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-clients
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:372)
[2021-12-08 12:54:08,758] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-clients
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:54:08,762] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.source.JdbcSourceTask
 (org.apache.kafka.connect.runtime.TaskConfig:372)
[2021-12-08 12:54:08,765] INFO Instantiated task jdbc-source-clients-0 with version 10.2.5 of type io.confluent.connect.jdbc.source.JdbcSourceTask (org.apache.kafka.connect.runtime.Worker:520)
[2021-12-08 12:54:08,768] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2021-12-08 12:54:08,782] INFO Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-source-clients-0 using the worker config (org.apache.kafka.connect.runtime.Worker:533)
[2021-12-08 12:54:08,822] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2021-12-08 12:54:08,831] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-source-clients-0 using the worker config (org.apache.kafka.connect.runtime.Worker:539)
[2021-12-08 12:54:08,834] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-source-clients-0 using the worker config (org.apache.kafka.connect.runtime.Worker:546)
[2021-12-08 12:54:08,864] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-clients
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:372)
[2021-12-08 12:54:08,866] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-clients
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:54:08,939] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:600)
[2021-12-08 12:54:08,998] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-jdbc-source-clients-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:372)
[2021-12-08 12:54:09,128] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-12-08 12:54:09,129] INFO Kafka version: 2.8.1 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-12-08 12:54:09,137] INFO Kafka commitId: 839b886f9b732b15 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-12-08 12:54:09,143] INFO Kafka startTimeMs: 1638968049129 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-12-08 12:54:09,264] INFO Starting JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:88)
[2021-12-08 12:54:09,265] INFO [Producer clientId=connector-producer-jdbc-source-clients-0] Cluster ID: u7MsxwDfR5Ob7FBWXv2wvA (org.apache.kafka.clients.Metadata:279)
[2021-12-08 12:54:09,267] INFO JdbcSourceTaskConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:mysql://localhost:3306/project3
	connection.user = projeto
	db.timezone = UTC
	dialect.name = MySqlDatabaseDialect
	incrementing.column.name = 
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 60000
	query = SELECT * FROM clients;
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = []
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	tables = []
	timestamp.column.name = []
	timestamp.delay.interval.ms = 0
	timestamp.initial = null
	topic.prefix = DBinfo
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceTaskConfig:372)
[2021-12-08 12:54:09,267] INFO Created connector jdbc-source-clients (org.apache.kafka.connect.cli.ConnectStandalone:109)
[2021-12-08 12:54:09,281] INFO Using JDBC dialect MySql (io.confluent.connect.jdbc.source.JdbcSourceTask:105)
[2021-12-08 12:54:09,287] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:372)
[2021-12-08 12:54:09,292] INFO Started JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:264)
[2021-12-08 12:54:09,304] INFO Creating connector jdbc-source-managers of type io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:271)
[2021-12-08 12:54:09,304] INFO WorkerSourceTask{id=jdbc-source-clients-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:232)
[2021-12-08 12:54:09,324] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-managers
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:372)
[2021-12-08 12:54:09,348] INFO WorkerSourceTask{id=jdbc-source-clients-0} Executing source task (org.apache.kafka.connect.runtime.WorkerSourceTask:238)
[2021-12-08 12:54:09,377] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-managers
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:54:09,447] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2021-12-08 12:54:09,484] INFO Instantiated connector jdbc-source-managers with version 10.2.5 of type class io.confluent.connect.jdbc.JdbcSourceConnector (org.apache.kafka.connect.runtime.Worker:281)
[2021-12-08 12:54:09,546] INFO Finished creating connector jdbc-source-managers (org.apache.kafka.connect.runtime.Worker:306)
[2021-12-08 12:54:09,588] INFO Starting JDBC Source Connector (io.confluent.connect.jdbc.JdbcSourceConnector:69)
[2021-12-08 12:54:09,615] INFO Begin using SQL query: SELECT * FROM clients; (io.confluent.connect.jdbc.source.TableQuerier:164)
[2021-12-08 12:54:09,634] INFO JdbcSourceConnectorConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:mysql://localhost:3306/project3
	connection.user = projeto
	db.timezone = UTC
	dialect.name = MySqlDatabaseDialect
	incrementing.column.name = 
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 60000
	query = SELECT * FROM managers;
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = []
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	timestamp.column.name = []
	timestamp.delay.interval.ms = 0
	timestamp.initial = null
	topic.prefix = DBinfo
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceConnectorConfig:372)
[2021-12-08 12:54:09,670] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2021-12-08 12:54:09,754] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-managers
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:372)
[2021-12-08 12:54:09,761] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-managers
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:54:09,805] INFO Creating task jdbc-source-managers-0 (org.apache.kafka.connect.runtime.Worker:505)
[2021-12-08 12:54:09,808] ERROR Non-transient SQL exception while running query for table: BulkTableQuerier{table='null', query='SELECT * FROM clients;', topicPrefix='DBinfo'} (io.confluent.connect.jdbc.source.JdbcSourceTask:412)
java.sql.SQLSyntaxErrorException: Table 'project3.clients' doesn't exist
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeQuery(ClientPreparedStatement.java:1009)
	at io.confluent.connect.jdbc.source.BulkTableQuerier.executeQuery(BulkTableQuerier.java:79)
	at io.confluent.connect.jdbc.source.TableQuerier.maybeStartQuery(TableQuerier.java:98)
	at io.confluent.connect.jdbc.source.BulkTableQuerier.maybeStartQuery(BulkTableQuerier.java:39)
	at io.confluent.connect.jdbc.source.JdbcSourceTask.poll(JdbcSourceTask.java:379)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.poll(WorkerSourceTask.java:296)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.execute(WorkerSourceTask.java:253)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
[2021-12-08 12:54:09,826] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-managers
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:372)
[2021-12-08 12:54:09,880] INFO Closing resources for JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:332)
[2021-12-08 12:54:09,891] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-managers
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:54:09,954] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2021-12-08 12:54:09,963] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.source.JdbcSourceTask
 (org.apache.kafka.connect.runtime.TaskConfig:372)
[2021-12-08 12:54:09,979] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:54:10,009] INFO Instantiated task jdbc-source-managers-0 with version 10.2.5 of type io.confluent.connect.jdbc.source.JdbcSourceTask (org.apache.kafka.connect.runtime.Worker:520)
[2021-12-08 12:54:10,041] ERROR WorkerSourceTask{id=jdbc-source-clients-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:190)
org.apache.kafka.connect.errors.ConnectException: java.sql.SQLSyntaxErrorException: Table 'project3.clients' doesn't exist
	at io.confluent.connect.jdbc.source.JdbcSourceTask.poll(JdbcSourceTask.java:417)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.poll(WorkerSourceTask.java:296)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.execute(WorkerSourceTask.java:253)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLSyntaxErrorException: Table 'project3.clients' doesn't exist
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeQuery(ClientPreparedStatement.java:1009)
	at io.confluent.connect.jdbc.source.BulkTableQuerier.executeQuery(BulkTableQuerier.java:79)
	at io.confluent.connect.jdbc.source.TableQuerier.maybeStartQuery(TableQuerier.java:98)
	at io.confluent.connect.jdbc.source.BulkTableQuerier.maybeStartQuery(BulkTableQuerier.java:39)
	at io.confluent.connect.jdbc.source.JdbcSourceTask.poll(JdbcSourceTask.java:379)
	... 9 more
[2021-12-08 12:54:10,051] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2021-12-08 12:54:10,054] INFO Stopping JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:320)
[2021-12-08 12:54:10,086] INFO Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-source-managers-0 using the worker config (org.apache.kafka.connect.runtime.Worker:533)
[2021-12-08 12:54:10,099] INFO Closing resources for JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:332)
[2021-12-08 12:54:10,116] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2021-12-08 12:54:10,127] INFO [Producer clientId=connector-producer-jdbc-source-clients-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1204)
[2021-12-08 12:54:10,139] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-source-managers-0 using the worker config (org.apache.kafka.connect.runtime.Worker:539)
[2021-12-08 12:54:10,174] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2021-12-08 12:54:10,182] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-source-managers-0 using the worker config (org.apache.kafka.connect.runtime.Worker:546)
[2021-12-08 12:54:10,188] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2021-12-08 12:54:10,195] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2021-12-08 12:54:10,195] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-managers
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:372)
[2021-12-08 12:54:10,196] INFO App info kafka.producer for connector-producer-jdbc-source-clients-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-12-08 12:54:10,201] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-source-managers
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:54:10,261] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:600)
[2021-12-08 12:54:10,287] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-jdbc-source-managers-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:372)
[2021-12-08 12:54:10,350] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2021-12-08 12:54:10,353] INFO Kafka version: 2.8.1 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-12-08 12:54:10,371] INFO Kafka commitId: 839b886f9b732b15 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-12-08 12:54:10,374] INFO Kafka startTimeMs: 1638968050352 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-12-08 12:54:10,383] INFO [Producer clientId=connector-producer-jdbc-source-managers-0] Cluster ID: u7MsxwDfR5Ob7FBWXv2wvA (org.apache.kafka.clients.Metadata:279)
[2021-12-08 12:54:10,384] INFO Starting JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:88)
[2021-12-08 12:54:10,384] INFO Created connector jdbc-source-managers (org.apache.kafka.connect.cli.ConnectStandalone:109)
[2021-12-08 12:54:10,388] INFO JdbcSourceTaskConfig values: 
	batch.max.rows = 100
	catalog.pattern = null
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:mysql://localhost:3306/project3
	connection.user = projeto
	db.timezone = UTC
	dialect.name = MySqlDatabaseDialect
	incrementing.column.name = 
	mode = bulk
	numeric.mapping = null
	numeric.precision.mapping = false
	poll.interval.ms = 60000
	query = SELECT * FROM managers;
	query.suffix = 
	quote.sql.identifiers = ALWAYS
	schema.pattern = null
	table.blacklist = []
	table.poll.interval.ms = 60000
	table.types = [TABLE]
	table.whitelist = []
	tables = []
	timestamp.column.name = []
	timestamp.delay.interval.ms = 0
	timestamp.initial = null
	topic.prefix = DBinfo
	validate.non.null = true
 (io.confluent.connect.jdbc.source.JdbcSourceTaskConfig:372)
[2021-12-08 12:54:10,450] INFO Using JDBC dialect MySql (io.confluent.connect.jdbc.source.JdbcSourceTask:105)
[2021-12-08 12:54:10,461] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:372)
[2021-12-08 12:54:10,472] INFO Started JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:264)
[2021-12-08 12:54:10,499] INFO Creating connector jdbc-sink of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:271)
[2021-12-08 12:54:10,501] INFO WorkerSourceTask{id=jdbc-source-managers-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:232)
[2021-12-08 12:54:10,513] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	topics = [results]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2021-12-08 12:54:10,514] INFO WorkerSourceTask{id=jdbc-source-managers-0} Executing source task (org.apache.kafka.connect.runtime.WorkerSourceTask:238)
[2021-12-08 12:54:10,531] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	topics = [results]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:54:10,552] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2021-12-08 12:54:10,575] INFO Instantiated connector jdbc-sink with version 10.2.5 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:281)
[2021-12-08 12:54:10,619] INFO Finished creating connector jdbc-sink (org.apache.kafka.connect.runtime.Worker:306)
[2021-12-08 12:54:10,636] INFO Begin using SQL query: SELECT * FROM managers; (io.confluent.connect.jdbc.source.TableQuerier:164)
[2021-12-08 12:54:10,648] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	topics = [results]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2021-12-08 12:54:10,652] ERROR Non-transient SQL exception while running query for table: BulkTableQuerier{table='null', query='SELECT * FROM managers;', topicPrefix='DBinfo'} (io.confluent.connect.jdbc.source.JdbcSourceTask:412)
java.sql.SQLSyntaxErrorException: Table 'project3.managers' doesn't exist
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeQuery(ClientPreparedStatement.java:1009)
	at io.confluent.connect.jdbc.source.BulkTableQuerier.executeQuery(BulkTableQuerier.java:79)
	at io.confluent.connect.jdbc.source.TableQuerier.maybeStartQuery(TableQuerier.java:98)
	at io.confluent.connect.jdbc.source.BulkTableQuerier.maybeStartQuery(BulkTableQuerier.java:39)
	at io.confluent.connect.jdbc.source.JdbcSourceTask.poll(JdbcSourceTask.java:379)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.poll(WorkerSourceTask.java:296)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.execute(WorkerSourceTask.java:253)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
[2021-12-08 12:54:10,665] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	topics = [results]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:54:10,694] INFO Closing resources for JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:332)
[2021-12-08 12:54:10,712] INFO Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector:44)
[2021-12-08 12:54:10,749] INFO Closing connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:105)
[2021-12-08 12:54:10,751] INFO Creating task jdbc-sink-0 (org.apache.kafka.connect.runtime.Worker:505)
[2021-12-08 12:54:10,766] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:54:10,767] ERROR WorkerSourceTask{id=jdbc-source-managers-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:190)
org.apache.kafka.connect.errors.ConnectException: java.sql.SQLSyntaxErrorException: Table 'project3.managers' doesn't exist
	at io.confluent.connect.jdbc.source.JdbcSourceTask.poll(JdbcSourceTask.java:417)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.poll(WorkerSourceTask.java:296)
	at org.apache.kafka.connect.runtime.WorkerSourceTask.execute(WorkerSourceTask.java:253)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.sql.SQLSyntaxErrorException: Table 'project3.managers' doesn't exist
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeQuery(ClientPreparedStatement.java:1009)
	at io.confluent.connect.jdbc.source.BulkTableQuerier.executeQuery(BulkTableQuerier.java:79)
	at io.confluent.connect.jdbc.source.TableQuerier.maybeStartQuery(TableQuerier.java:98)
	at io.confluent.connect.jdbc.source.BulkTableQuerier.maybeStartQuery(BulkTableQuerier.java:39)
	at io.confluent.connect.jdbc.source.JdbcSourceTask.poll(JdbcSourceTask.java:379)
	... 9 more
[2021-12-08 12:54:10,794] INFO Stopping JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:320)
[2021-12-08 12:54:10,767] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:372)
[2021-12-08 12:54:10,807] INFO Closing resources for JDBC source task (io.confluent.connect.jdbc.source.JdbcSourceTask:332)
[2021-12-08 12:54:10,813] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:54:10,827] INFO [Producer clientId=connector-producer-jdbc-source-managers-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1204)
[2021-12-08 12:54:10,853] INFO TaskConfig values: 
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:372)
[2021-12-08 12:54:10,878] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2021-12-08 12:54:10,895] INFO Instantiated task jdbc-sink-0 with version 10.2.5 of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker:520)
[2021-12-08 12:54:10,898] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2021-12-08 12:54:10,932] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:372)
[2021-12-08 12:54:10,935] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2021-12-08 12:54:10,938] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:372)
[2021-12-08 12:54:10,975] INFO App info kafka.producer for connector-producer-jdbc-source-managers-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2021-12-08 12:54:10,983] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task jdbc-sink-0 using the connector config (org.apache.kafka.connect.runtime.Worker:535)
[2021-12-08 12:54:11,017] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-sink-0 using the connector config (org.apache.kafka.connect.runtime.Worker:541)
[2021-12-08 12:54:11,022] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:546)
[2021-12-08 12:54:11,028] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:626)
[2021-12-08 12:54:11,029] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	topics = [results]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:372)
[2021-12-08 12:54:11,044] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	topics = [results]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:372)
[2021-12-08 12:54:11,086] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-jdbc-sink-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-jdbc-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:372)
[2021-12-08 12:54:11,206] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2021-12-08 12:54:11,207] INFO Kafka version: 2.8.1 (org.apache.kafka.common.utils.AppInfoParser:119)
[2021-12-08 12:54:11,214] INFO Kafka commitId: 839b886f9b732b15 (org.apache.kafka.common.utils.AppInfoParser:120)
[2021-12-08 12:54:11,215] INFO Kafka startTimeMs: 1638968051207 (org.apache.kafka.common.utils.AppInfoParser:121)
[2021-12-08 12:54:11,224] INFO Created connector jdbc-sink (org.apache.kafka.connect.cli.ConnectStandalone:109)
[2021-12-08 12:54:11,225] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Subscribed to topic(s): results (org.apache.kafka.clients.consumer.KafkaConsumer:965)
[2021-12-08 12:54:11,226] INFO Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask:48)
[2021-12-08 12:54:11,227] INFO JdbcSinkConfig values: 
	auto.create = true
	auto.evolve = false
	batch.size = 3000
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:mysql://localhost:3306/project3
	connection.user = projeto
	db.timezone = UTC
	delete.enabled = false
	dialect.name = MySqlDatabaseDialect
	fields.whitelist = [profit, revenue]
	insert.mode = insert
	max.retries = 10
	pk.fields = []
	pk.mode = none
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = ola
	table.types = [TABLE]
 (io.confluent.connect.jdbc.sink.JdbcSinkConfig:372)
[2021-12-08 12:54:11,237] INFO Initializing writer using SQL dialect: MySqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask:67)
[2021-12-08 12:54:11,266] INFO WorkerSinkTask{id=jdbc-sink-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:310)
[2021-12-08 12:54:11,269] INFO WorkerSinkTask{id=jdbc-sink-0} Executing sink task (org.apache.kafka.connect.runtime.WorkerSinkTask:196)
[2021-12-08 12:54:11,378] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Cluster ID: u7MsxwDfR5Ob7FBWXv2wvA (org.apache.kafka.clients.Metadata:279)
[2021-12-08 12:54:11,383] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Discovered group coordinator Nuno-PC:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:850)
[2021-12-08 12:54:11,390] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-12-08 12:54:11,469] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:540)
[2021-12-08 12:54:15,465] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Successfully joined group with generation Generation{generationId=8, memberId='connector-consumer-jdbc-sink-0-5ec5afe4-5e82-4ff3-835c-ce2e10688290', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:596)
[2021-12-08 12:54:15,483] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Finished assignment for group at generation 8: {connector-consumer-jdbc-sink-0-5ec5afe4-5e82-4ff3-835c-ce2e10688290=Assignment(partitions=[results-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:626)
[2021-12-08 12:54:15,672] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Successfully synced group in generation Generation{generationId=8, memberId='connector-consumer-jdbc-sink-0-5ec5afe4-5e82-4ff3-835c-ce2e10688290', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:760)
[2021-12-08 12:54:15,674] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Notifying assignor about the new Assignment(partitions=[results-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2021-12-08 12:54:15,683] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Adding newly assigned partitions: results-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2021-12-08 12:54:15,787] INFO [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Setting offset for partition results-0 to the committed offset FetchPosition{offset=6, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[Nuno-PC:9092 (id: 0 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:820)
[2021-12-08 12:54:19,265] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:54:20,397] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:54:25,651] INFO Attempting to open connection #1 to MySql (io.confluent.connect.jdbc.util.CachedConnectionProvider:79)
[2021-12-08 12:54:25,659] INFO JdbcDbWriter Connected (io.confluent.connect.jdbc.sink.JdbcDbWriter:56)
[2021-12-08 12:54:25,681] INFO Checking MySql dialect for existence of TABLE "ola" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:575)
[2021-12-08 12:54:25,692] INFO Using MySql dialect TABLE "ola" absent (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:583)
[2021-12-08 12:54:25,695] INFO Creating table with sql: CREATE TABLE `ola` (
`revenue` DOUBLE NOT NULL,
`profit` DOUBLE NOT NULL) (io.confluent.connect.jdbc.sink.DbStructure:122)
[2021-12-08 12:54:25,728] INFO Checking MySql dialect for existence of TABLE "ola" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:575)
[2021-12-08 12:54:25,741] INFO Using MySql dialect TABLE "ola" present (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:583)
[2021-12-08 12:54:25,773] INFO Checking MySql dialect for type of TABLE "ola" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:845)
[2021-12-08 12:54:25,786] INFO Setting metadata for table "ola" to Table{name='"ola"', type=TABLE columns=[Column{'revenue', isPrimaryKey=false, allowsNull=false, sqlType=DOUBLE}, Column{'profit', isPrimaryKey=false, allowsNull=false, sqlType=DOUBLE}]} (io.confluent.connect.jdbc.util.TableDefinitions:64)
[2021-12-08 12:54:29,270] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:54:30,408] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:54:39,280] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:54:40,410] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:54:49,287] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:54:50,415] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:54:59,289] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:55:00,423] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:55:09,295] INFO WorkerSourceTask{id=jdbc-source-clients-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
[2021-12-08 12:55:10,424] INFO WorkerSourceTask{id=jdbc-source-managers-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:510)
